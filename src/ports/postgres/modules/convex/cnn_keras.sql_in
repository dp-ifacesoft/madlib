/* ----------------------------------------------------------------------- *//**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 *
 * @file mlp.sql_in
 *
 * @brief SQL functions for multilayer perceptron
 * @date June 2012
 *
 *
 *//* ----------------------------------------------------------------------- */

m4_include(`SQLCommon.m4')

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.run_keras(
    source_table text,
    test_table text,
    model_table text,
    n_iter  int
)
RETURNS VOID AS $$
    N_ITERATIONS = n_iter

    import plpy
    import keras
    import numpy as np
    import json
    import pandas as pd
    import time
    import os
    import sys

    from keras.models import Sequential
    from keras.datasets import mnist, cifar10
    from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, ZeroPadding2D, Activation
    from keras.optimizers import Adam, SGD
    from keras import backend as keras_backend

    n_classes = 26

    def create_model_architecture(n_classes):
        model = Sequential()

        model.add(ZeroPadding2D((1, 1), input_shape=(224,224,3)))
        model.add(Conv2D(64, (3, 3)))
        model.add(Activation('relu'))
        model.add(ZeroPadding2D((1, 1)))
        model.add(Conv2D(64, (3, 3)))
        model.add(Activation('relu'))
        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

        model.add(ZeroPadding2D((1, 1)))
        model.add(Conv2D(128, (3, 3)))
        model.add(Activation('relu'))
        model.add(ZeroPadding2D((1, 1)))
        model.add(Conv2D(128, (3, 3)))
        model.add(Activation('relu'))
        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

        model.add(ZeroPadding2D((1, 1)))
        model.add(Conv2D(256, (3, 3)))
        model.add(Activation('relu'))
        model.add(ZeroPadding2D((1, 1)))
        model.add(Conv2D(256, (3, 3)))
        model.add(Activation('relu'))
        model.add(ZeroPadding2D((1, 1)))
        model.add(Conv2D(256, (3, 3)))
        model.add(Activation('relu'))
        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

        model.add(ZeroPadding2D((1, 1)))
        model.add(Conv2D(512, (3, 3)))
        model.add(Activation('relu'))
        model.add(ZeroPadding2D((1, 1)))
        model.add(Conv2D(512, (3, 3)))
        model.add(Activation('relu'))
        model.add(ZeroPadding2D((1, 1)))
        model.add(Conv2D(512, (3, 3)))
        model.add(Activation('relu'))
        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

        model.add(ZeroPadding2D((1, 1)))
        model.add(Conv2D(512, (3, 3)))
        model.add(Activation('relu'))
        model.add(ZeroPadding2D((1, 1)))
        model.add(Conv2D(512, (3, 3)))
        model.add(Activation('relu'))
        model.add(ZeroPadding2D((1, 1)))
        model.add(Conv2D(512, (3, 3)))
        model.add(Activation('relu'))
        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

        model.add(Flatten())
        model.add(Dense(26))
        model.add(Activation('softmax'))
        sgd = SGD(lr=0.01, decay=1e-6, nesterov=True)
        # model.summary()

        model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])

#        model = Sequential()
#
#        # This was the working network for 1-D mnist dataset
#        model.add(Dense(100, activation='relu', input_shape=(40,40,1)))
#        model.add(Dropout(0.2))
#        model.add(Flatten())
#        model.add(Dense(n_classes, activation='softmax'))
#
#        # This was the working network for CIFAR-10 dataset
#        #model.add(Conv2D(32, kernel_size=(3, 3),
#        #          activation='relu',
#        #          input_shape=(40,40,1,)))
#        #model.add(MaxPooling2D(pool_size=(2, 2)))
#        #model.add(Dropout(0.25))
#        #model.add(Flatten())
#        #model.add(Dense(n_classes, activation='softmax'))

#        model.compile(loss='categorical_crossentropy',
#                      optimizer=Adam(),
#                      metrics=['categorical_accuracy'])
        return model

    model = create_model_architecture(n_classes)
    ##########################################################################
    # These variables are for model evaluation
    #(x_train, y_train), (x_test, y_test) = cifar10.load_data()
    #path='/home/gpadmin/training_and_validation_batches'
    #data_key = 'affNISTdata'
    #all_x_train = np.ndarray((0,1600))
    #all_y_train = np.ndarray((0,1))

#    for i in range(1,20):
#    for i in range(1,2):
#        f = str(i) + '.mat'
#        #plpy.info('file {0} being processed'.format(f))
#        full_path = os.path.join(path, f)
#        train_batch=mat.loadmat(full_path)
#        x_train = train_batch[data_key][0][0][2].transpose()
#        y_train = train_batch[data_key][0][0][5].transpose()
#        all_x_train=np.concatenate((all_x_train, x_train))
#        all_y_train=np.concatenate((all_y_train, y_train))

#    num_train_examples = all_x_train.shape[0]
#    all_x_train = all_x_train.reshape(num_train_examples, 40,40,1)
#    all_x_train = all_x_train.astype('float32')
#    all_x_train /= 255.0
#    all_y_train = keras.utils.to_categorical(all_y_train, n_classes)
#    plpy.info('Train Table : {0} **********************************************************'.format(source_table))
#    plpy.info('X shape : {0}'.format(all_x_train.shape))
#    plpy.info('Y shape : {0}'.format(all_y_train.shape))
#    agg_scores = []
#    agg_scores.append(model.evaluate(all_x_train, all_y_train, verbose=0)[1])

    ###########################################################################

    #sql_test_x = """
    #    SELECT independent_varname as x FROM {0}
    #    """.format(test_table)
    #plpy.info('Test sql is : {0}'.format(sql_test_x))
    #testDataX = plpy.execute(sql_test_x)

    #sql_test_y = """
    #    SELECT dependent_varname as y FROM {0}
    #    """.format(test_table)
    #plpy.info('Test sql is : {0}'.format(sql_test_y))
    #testDataY = plpy.execute(sql_test_y)

    #all_x_train = np.ndarray((0,1600))
    #all_y_train = np.ndarray((0,1))
    #for i in range(len(testDataX)):
    #    x_train = np.asarray(testDataX[i]['x'])
    #    y_train = np.asarray(testDataY[i]['y'])
    #    plpy.info('i : {0}'.format(i))
    #    plpy.info('X shape : {0}'.format(x_train.shape))
    #    plpy.info('Y shape : {0}'.format(y_train.shape))
    #    all_x_train=np.concatenate((all_x_train, x_train))
    #    all_y_train=np.concatenate((all_y_train, y_train))

    # img = all_x_train[13000].reshape(40,40)
    # misc.imsave('/tmp/output.jpg',img)
    # num_train_examples = all_x_train.shape[0]
    # all_x_train = all_x_train.reshape(num_train_examples, 40,40,1)
    # all_x_train = all_x_train.astype('float32')
    # all_y_train = all_y_train.reshape(num_train_examples)
    # plpy.info('X shape : {0}'.format(all_x_train.shape))
    # plpy.info('Y shape : {0}'.format(all_y_train.shape))
    # all_x_train /= 255.0
    # all_y_train = keras.utils.to_categorical(all_y_train, n_classes)
    # agg_scores = []
    # agg_scores.append(model.evaluate(all_x_train, all_y_train, verbose=0)[1])

    ###########################################################################

    #sql_test_x = """
    #    SELECT independent_varname as x, dependent_varname as y FROM {0}
    #    """.format(test_table)
    #plpy.info('Test sql is : {0}'.format(sql_test_x))
    #testData = plpy.execute(sql_test_x)


    #all_x_train = np.ndarray((0,150528))
    #all_y_train = np.ndarray((0,1))
    #for i in range(len(testData)):
    #    x_train = np.asarray(testData[i]['x'])
    #    y_train = np.asarray(testData[i]['y'])
    #    #plpy.info('i : {0}'.format(i))
    #    #plpy.info('X shape : {0}'.format(x_train.shape))
    #    #plpy.info('Y shape : {0}'.format(y_train.shape))
    #    all_x_train=np.concatenate((all_x_train, x_train))
    #    all_y_train=np.concatenate((all_y_train, y_train))

    #num_train_examples = all_x_train.shape[0]
    #all_x_train = all_x_train.reshape(num_train_examples, 224,224,3)
    #all_x_train = all_x_train.astype('float32')
    #all_y_train = all_y_train.reshape(num_train_examples)
    #plpy.info('X shape : {0}'.format(all_x_train.shape))
    #plpy.info('Y shape : {0}'.format(all_y_train.shape))
    #all_x_train /= 255.0
    #all_y_train = keras.utils.to_categorical(all_y_train, n_classes)
    #agg_scores = []
    #eval_start = time.time()
    #agg_scores.append(model.evaluate(all_x_train, all_y_train, verbose=0)[1])
    #eval_time = time.time()
    #plpy.info("time to evaluate model = {0}".format(eval_time-eval_start))
    #plpy.info(agg_scores)

    sql = """
        SELECT MADLIB_SCHEMA.cnn_keras_step(
            independent_varname::DOUBLE PRECISION[],
            dependent_varname::DOUBLE PRECISION[],
            $MAD${0}$MAD$::text,
            $1
        ) AS keras_model
        FROM {1}
        """.format(model.to_json(), source_table)

    update_plan = plpy.prepare(sql, ["JSON"])
    plpy.info("model arch is in place")
    #model_weights = [a.shape for a in model.get_weights()]
    #plpy.info(model_weights)
    model_weights = json.dumps([a.tolist() for a in model.get_weights()])
    plpy.info(sys.getsizeof(model_weights))
    start_time = time.time()
    for i in range(N_ITERATIONS):
        iter_start_time = time.time()
        try:
            model_weights = plpy.execute(update_plan, [model_weights])[0]['keras_model']
        except plpy.SPIError as e:
            plpy.notice(e)
            plpy.error('plpy exception')
        iter_endtime = time.time()
        plpy.info('Time for iteration {0}:{1}'.format(i, iter_endtime-iter_start_time))
        plpy.info('Time elapsed since start of iter 0: {1}'.format(i, iter_endtime-start_time))
        model_eval = create_model_architecture(n_classes)
        model_eval.set_weights([np.array(a) for a in json.loads(model_weights)])
        agg_score = model_eval.evaluate(all_x_train, all_y_train, verbose=0)[1]
        #agg_score = model_eval.evaluate(x_train, y_train, verbose=0)[1]
        plpy.info("Training categorical_accuracy in iteration {0} = {1}".format(i, agg_score))
        agg_scores.append(agg_score)

    plan = plpy.prepare("""
        drop table if exists {0};
        CREATE TABLE {0} AS
        SELECT $1 AS keras_model,
        $2 as accuracy_history
        """.format(model_table), ["JSON", "DOUBLE PRECISION[]"])
    plpy.execute(plan, [model_weights, agg_scores])


$$ language plpythonu;


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.cnn_keras_transition(
    state               JSON,
    ind_var             DOUBLE PRECISION[],
    dep_var             DOUBLE PRECISION[],
    architecture        TEXT,
    previous_state      JSON
) RETURNS JSON AS
$$
import itertools
import numpy as np

import plpy
import keras
import json
import time
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, ZeroPadding2D, Activation
from keras.optimizers import Adam, SGD
from keras.models import model_from_json
from keras import backend as keras_backend

if not state:
    # model needs to be initialized
    serialized_state = previous_state
else:
    serialized_state = state
keras_model = model_from_json(architecture)
#plpy.info("Transition STARTED")
start = time.time()
keras_model.set_weights([np.array(a) for a in json.loads(serialized_state)])
# We must run model.compile again, after obtaining the architecutre.
keras_model.compile(optimizer=SGD(lr=0.01, decay=1e-6, nesterov=True),
    loss='categorical_crossentropy',
    metrics=['accuracy'])
#keras_model.compile(loss='categorical_crossentropy',
#              optimizer=Adam(),
#              metrics=['categorical_accuracy'])
if ind_var == None:
   return None
#plpy.info("buffer len {0} {1}".format(len(ind_var), len(dep_var)))
x_train = np.array(ind_var).reshape(len(ind_var), 224,224,3)
# Figure out why this is necessary, but IT IS necessary!
x_train /= 255.0

y_train = np.array(dep_var)
y_train = keras.utils.to_categorical(y_train, 26)
#plpy.info("y train shape is {0}".format(y_train.shape))
mid = time.time()
plpy.info("will try to fit now")
keras_model.fit(x_train, y_train,
               batch_size=32,
               epochs=1,
               verbose=0)
end = time.time()
plpy.info("done with fit")
weights_in_list = [a.tolist() for a in keras_model.get_weights()]
weights_in_json = json.dumps(weights_in_list)
before_return = time.time()
plpy.info("times = {0}, {1}, {2}".format(mid-start, end-mid, before_return-end))
keras_backend.clear_session()
#plpy.info("TRANSITION done.")
return weights_in_json

$$
LANGUAGE plpythonu
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `NO SQL', `');


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.cnn_keras_merge(
    state1          JSON,
    state2          JSON
) RETURNS JSON AS
$$
import json
import numpy as np

if not state1 or not state2:
    return state1 or state2
weights1 = [np.array(a) for a in json.loads(state1)]
weights2 = [np.array(a) for a in json.loads(state2)]
## This is a simple merge where we assume each segment has *equal* number of
## rows. If it is not the case, we must do a weighted avg, and should store
## the number of rows info somewhere.
merged_weights = [((a+b)).tolist() for a, b in zip(weights1, weights2)]
return json.dumps(merged_weights)
$$
LANGUAGE plpythonu
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `NO SQL', `');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.cnn_keras_final(
    state JSON
) RETURNS JSON AS
$$
import json
import numpy as np
weights1 = [np.array(a) for a in json.loads(state)]
normalized_weights=[(a/1).tolist() for a in weights1]
return json.dumps(normalized_weights)
$$
LANGUAGE plpythonu
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `NO SQL', `');

CREATE AGGREGATE MADLIB_SCHEMA.cnn_keras_step(
    /* ind_var */             DOUBLE PRECISION[],
    /* dep_var */             DOUBLE PRECISION[],
    /* architecture */        TEXT,
    /* previous_state */      JSON
)(
    STYPE=JSON,
    SFUNC=MADLIB_SCHEMA.cnn_keras_transition,
    PREFUNC=MADLIB_SCHEMA.cnn_keras_merge,
    FINALFUNC=MADLIB_SCHEMA.cnn_keras_final
);
