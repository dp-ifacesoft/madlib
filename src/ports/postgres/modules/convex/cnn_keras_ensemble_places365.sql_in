/* ----------------------------------------------------------------------- *//**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 *
 * @file mlp.sql_in
 *
 * @brief SQL functions for multilayer perceptron
 * @date June 2012
 *
 *
 *//* ----------------------------------------------------------------------- */

m4_include(`SQLCommon.m4')

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.run_keras(
    source_table text,
    test_table text,
    model_table text,
    n_iter  int
)
RETURNS VOID AS $$
    N_ITERATIONS = n_iter

    import plpy
    import keras
    import numpy as np
    import pandas as pd
    import time
    import os
    import sys
    import gc
    import random

    from keras.models import Sequential
    from keras.datasets import mnist, cifar10
    from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, ZeroPadding2D, Activation, BatchNormalization
    from keras.optimizers import Adam, SGD
    from keras import backend as K
    from keras.regularizers import l2
    from keras.layers import Input
    from keras.applications.imagenet_utils import _obtain_input_shape
    from keras.models import Model
    from keras.models import model_from_json, clone_model
    from keras.callbacks import EarlyStopping

    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # to disable GPU

    n_classes = 365

    def deserialize(state, model_shapes):
        weightsList = []
        j = 0
        for a in filter(None, state.split('splitter')[1:]):
            arr = np.fromstring(a, dtype=np.float32)
            weightsList.append(arr.reshape(model_shapes[j]))
            j += 1
        return weightsList

    def deserialize_modelagg(state):
        model_list = state.split('modelsplitarvind')
        return model_list

    def serialized_modelagg(model_list):
        model_list_serialized = ""
        for model in model_list:
            if model_list_serialized:
                model_list_serialized += ("modelsplitarvind" + model)
            else:
                model_list_serialized += model
        return model_list_serialized

    def alexnet_construct(n_classes, config='A'):
        model = Sequential()

        # 1st Convolutional Layer
        model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),\
                 strides=(4,4), padding='valid'))
        model.add(Activation('relu'))
        # Pooling
        model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))
        # Batch Normalisation before passing it to the next layer
        model.add(BatchNormalization())

        # 2nd Convolutional Layer
        model.add(Conv2D(filters=128, kernel_size=(11,11), strides=(1,1), padding='valid'))
        model.add(Activation('relu'))
        # Pooling
        model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))
        # Batch Normalisation
        model.add(BatchNormalization())

        # 3rd Convolutional Layer
        model.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding='valid'))
        model.add(Activation('relu'))
        # Batch Normalisation
        model.add(BatchNormalization())

        # 4th Convolutional Layer
        model.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding='valid'))
        model.add(Activation('relu'))
        # Batch Normalisation
        model.add(BatchNormalization())

        # 5th Convolutional Layer
        model.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding='valid'))
        model.add(Activation('relu'))
        # Pooling
        model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))
        # Batch Normalisation
        model.add(BatchNormalization())

        # Passing it to a dense layer
        model.add(Flatten())
        # 1st Dense Layer
        model.add(Dense(2048, input_shape=(224*224*3,)))
        model.add(Activation('relu'))
        # Add Dropout to prevent overfitting
        model.add(Dropout(0.4))
        # Batch Normalisation
        model.add(BatchNormalization())

        # 2nd Dense Layer
        model.add(Dense(2048))
        model.add(Activation('relu'))
        # Add Dropout
        model.add(Dropout(0.4))
        # Batch Normalisation
        model.add(BatchNormalization())

        # 3rd Dense Layer
        model.add(Dense(500))
        model.add(Activation('relu'))
        # Add Dropout
        model.add(Dropout(0.4))
        # Batch Normalisation
        model.add(BatchNormalization())

        # Output Layer
        model.add(Dense(n_classes))
        model.add(Activation('softmax'))

        # Compile and Return
        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
        return model

    master_model = alexnet_construct(n_classes)

    sql_test_x = """
    SELECT x, y FROM {0} ORDER BY random() LIMIT 200""".format(test_table)
    plpy.info('Test sql is : {0}'.format(sql_test_x))
    testData = plpy.execute(sql_test_x)
    plpy.info('finished running select query')
    after_select = time.time();

    all_x_test = np.ndarray((0, 5, 150528))
    all_y_test = np.ndarray((0, 5))
    for i in range(len(testData)):
        x_test = np.asarray((testData[i]['x'],))
        y_test = np.asarray((testData[i]['y'],))
        plpy.info('i : {0}'.format(i))
        if i == 0:
            plpy.info('X shape : {0}'.format(x_test.shape))
            plpy.info('Y shape : {0}'.format(y_test.shape))
        all_x_test=np.concatenate((all_x_test, x_test))
        all_y_test=np.concatenate((all_y_test, y_test))

    num_test_examples = all_x_test.shape[0]
    all_x_test = all_x_test.reshape(5*num_test_examples, 224, 224, 3)
    all_x_test = all_x_test.astype('float32')
    all_y_test = all_y_test.reshape(5*num_test_examples)
    plpy.info('X shape : {0}'.format(all_x_test.shape))
    plpy.info('Y shape : {0}'.format(all_y_test.shape))
    all_x_test /= 255.0
    all_y_test = keras.utils.to_categorical(all_y_test,n_classes)

    eval_start = time.time()
    plpy.info("Time to prepare test data = {}".format(eval_start - after_select))
    agg_score = master_model.evaluate(all_x_test, all_y_test,verbose=0)[1]
    eval_time = time.time()
    plpy.info("Time to evaluate model = {0}".format(eval_time-eval_start))
    plpy.info("Accuracy before any training = {0}".format(agg_score))

    agg_scores = []
    model_arch = master_model.to_json()
    sql = """
        SELECT MADLIB_SCHEMA.cnn_keras_step(
            x::REAL[],
            y::SMALLINT[],
            gp_segment_id,
            $MAD${0}$MAD$::text,
            $1
        ) AS keras_model
        FROM {1}
        """.format(model_arch, source_table)
    plpy.info("Model arch size: {}KB".format(len(model_arch)/1024))
    update_plan = plpy.prepare(sql, ["bytea"])
    plpy.info("model arch is in place")

    model_list_serialized = ""
    model_shapes = []
    for i in range(20): # 20 = num segments
        model_string = "0splitter"
        start_time = time.time()
        for a in master_model.get_weights():
            model_string += a.tostring()
            model_string += 'splitter'
            if i == 0:
                model_shapes.append(a.shape)
        if i != 0:
            model_string = "modelsplitarvind" + model_string
        model_list_serialized += model_string
    plpy.info("Model state size: {}MB".format(len(model_list_serialized)/1024/1024))

    start_time = time.time()
    for i in range(N_ITERATIONS):
        iter_start_time = time.time()
        try:
            model_list_serialized = plpy.execute(update_plan, [model_list_serialized])[0]['keras_model']
        except plpy.SPIError as e:
            plpy.notice(e)
            plpy.error('plpy exception')
        iter_endtime = time.time()
        plpy.info('Time for iteration {0}: {1}'.format(i+1, iter_endtime-iter_start_time))

        # Shuffle the model ensemble for next iteration
        model_list = deserialize_modelagg(model_list_serialized)
        weightsList = deserialize(model_list[0], model_shapes)
        random.shuffle(model_list)
        model_list_serialized = serialized_modelagg(model_list)

        # Evaluate the first model
        master_model.set_weights(weightsList)
        agg_score = master_model.evaluate(all_x_test, all_y_test, verbose=0)[1]
        plpy.info("Training categorical accuracy after iteration {0}: {1}".format(i+1, agg_score))
        agg_scores.append(agg_score)

    # Construct, train, test the ensemble
    def get_ensemble_preds(ensemble, x_input):
        ensemble_preds = []
        for model in ensemble:
            prediction = model.predict(x_input, verbose=1).T
            ensemble_preds.append(prediction)
        return np.array(ensemble_preds).T.reshape((x_input.shape[0], 20, n_classes, 1))
    # Shuffle the test dataset
    rng_state = np.random.get_state()
    np.random.shuffle(all_x_test)
    np.random.set_state(rng_state)
    np.random.shuffle(all_y_test)
    plpy.info(np.argmax(all_y_test[1:100], axis=1))
    # Split test dataset into ensemble train and test sets
    split = int(0.7*all_x_test.shape[0])
    x_train_ensemble = all_x_test[0:split]
    y_train_ensemble = all_y_test[0:split]
    x_test_ensemble = all_x_test[split:]
    y_test_ensemble = all_y_test[split:]
    plpy.info("x train ensemble shape: {}".format(x_train_ensemble.shape))
    plpy.info("x test ensemble shape: {}".format(x_test_ensemble.shape))
    conv_start = time.time()
    # Construct the ensemble models
    model_list = deserialize_modelagg(model_list_serialized)
    ensemble = []
    for model in model_list:
        weightsList = deserialize(model, model_shapes)
        seg_model = clone_model(master_model)
        seg_model.set_weights(weightsList)
        ensemble.append(seg_model)
    plpy.info("Time for ensemble construction: {} sec".format(time.time() - conv_start))
    # Get predictions from ensemble models
    ensemble_preds_train = get_ensemble_preds(ensemble, x_train_ensemble)
    ensemble_preds_test = get_ensemble_preds(ensemble, x_test_ensemble)
    # Construct and train the ensemble model
    conv_ensemble_model = Sequential()
    conv_ensemble_model.add(Conv2D(256, kernel_size=(3, 3),
        activation='relu',
        input_shape=(20, n_classes, 1,)))
    conv_ensemble_model.add(Conv2D(256, (3, 3), activation='relu'))
    conv_ensemble_model.add(MaxPooling2D(pool_size=(2, 2)))
    conv_ensemble_model.add(Dropout(0.55))
    conv_ensemble_model.add(Flatten())
    conv_ensemble_model.add(Dense(512, activation='relu'))
    conv_ensemble_model.add(Dropout(0.55))
    conv_ensemble_model.add(Dense(n_classes, activation='softmax'))
    conv_ensemble_model.compile(loss='categorical_crossentropy',
        optimizer=Adam(),
        metrics=['accuracy'])
    history = conv_ensemble_model.fit(ensemble_preds_train, y_train_ensemble,
        batch_size=15,
        epochs=60,
        verbose=1,
        callbacks=[EarlyStopping(monitor='loss', patience=10, verbose=0)])
    # Evaluate the ensemble model
    train_score = conv_ensemble_model.evaluate(ensemble_preds_train, y_train_ensemble)
    test_score = conv_ensemble_model.evaluate(ensemble_preds_test, y_test_ensemble)
    plpy.info("Time for ensemble iteration: {0}".format(time.time() - conv_start))
    plpy.info("Convolutional ensemble train accuracy iteration: {0}".format(train_score[1]))
    plpy.info("Convolutional ensemble test accuracy iteration: {0}".format(test_score[1]))

    plan = plpy.prepare("""
        drop table if exists {0};
        CREATE TABLE {0} AS
        SELECT $1 AS keras_model,
        $2 as accuracy_history
        """.format(model_table), ["text", "DOUBLE PRECISION[]"])
    plpy.execute(plan, [weightsList, agg_scores])


$$ language plpythonu;


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.cnn_keras_transition(
    state               BYTEA,
    ind_var             REAL[],
    dep_var             SMALLINT[],
    seg                 INTEGER,
    architecture        TEXT,
    previous_state      BYTEA
) RETURNS BYTEA AS
$$

import plpy
import os

gpus_per_host=4
device_name = '/gpu:0'
os.environ["CUDA_VISIBLE_DEVICES"] = str(seg % gpus_per_host)

import numpy as np
import keras
import time
import gc
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, ZeroPadding2D, Activation
from keras.optimizers import Adam, SGD
from keras.models import model_from_json
from keras import backend as K
from tensorflow.python.client import device_lib

def transition_deserialize(state, model_shapes):
    weightsList = []
    j = 0
    split_state = filter(None, state.split('splitter'))
    for a in split_state[1:]:
        arr = np.fromstring(a, dtype=np.float32)
        weightsList.append(arr.reshape(model_shapes[j]))
        j += 1
    return weightsList

def transition_deserialize_modelagg(model_list, model_shapes):
    weightsList = []
    j = 0
    state = model_list.split("modelsplitarvind")[seg]
    split_state = filter(None, state.split('splitter'))
    for a in split_state[1:]:
        arr = np.fromstring(a, dtype=np.float32)
        weightsList.append(arr.reshape(model_shapes[j]))
        j += 1
    return int(split_state[0]), weightsList

def transition_serialize_modelagg(model_list):
    model_list_serialized = ""
    for model in model_list:
        if model_list_serialized:
            model_list_serialized += ("modelsplitarvind" + model)
        else:
            model_list_serialized += model
    return model_list_serialized

if ind_var == None:
   return None
buffer_count = None

start_transition = time.time()
if not state:
    serialized_state = previous_state
    config = K.tf.ConfigProto()
    config.gpu_options.allow_growth = False
    config.gpu_options.per_process_gpu_memory_fraction = 0.9
    session = K.tf.Session(config=config)
    K.set_session(session)
    if 'keras_model' in SD:
        del SD['keras_model']
    if 'model_shapes' in SD:
        del SD['model_shapes']
    SD['buffer_count'] = 0
else:
    serialized_state = state

if 'buffer_count' in SD:
    buffer_count = SD['buffer_count']

if 'keras_model' in SD:
    keras_model = SD['keras_model']
else:
    plpy.info("Decoding model arch from json and compiling.")
    keras_model = model_from_json(architecture)
    SD['keras_model'] = keras_model
    with K.tf.device(device_name):
        keras_model.compile(optimizer=SGD(lr=0.01, decay=1e-6, nesterov=True),
            loss='categorical_crossentropy', metrics=['accuracy'])

if 'model_shapes' in SD:
    model_shapes = SD['model_shapes']
else:
    plpy.info("Rebuilding model_shapes from get_weights.")
    model_shapes = []
    with K.tf.device(device_name):
        for a in keras_model.get_weights():
            model_shapes.append(a.shape)
    plpy.info("model shapes size: {}".format(len(model_shapes)))
    SD['model_shapes'] = model_shapes

if buffer_count is None or state is None:
    buffer_count, weightsList = transition_deserialize_modelagg(serialized_state, model_shapes)
    with K.tf.device(device_name):
        keras_model.set_weights(weightsList)

x_train = np.array(ind_var).reshape(len(ind_var), 224, 224, 3)
# Figure out why this is necessary, but IT IS necessary!
x_train /= 256 # should this be 255
y_train = np.array(dep_var)
plpy.info("Max label on segment {} on buffer {}: {}".format(buffer_count+1, seg, max(y_train)))
n_classes = 365
y_train = keras.utils.to_categorical(y_train, n_classes)

start_fit = time.time()
with K.tf.device(device_name):
    keras_model.fit(x_train, y_train,
       batch_size=25,
       epochs=1,
       verbose=0)
end_fit = time.time()

buffer_count += 1
SD['buffer_count'] = buffer_count
total_buffers=185

with K.tf.device(device_name):
    weights = keras_model.get_weights()
model_string = str(buffer_count) + 'splitter'
for a in weights:
    model_string += a.tostring()
    model_string += "splitter"

start_closing_session = time.time()
if buffer_count == total_buffers:
    sess = K.get_session()
    K.clear_session()
    sess.close()
end_closing_session = time.time()

del x_train
del y_train

end_transition = time.time()

plpy.info("Processed buffer {}. Before {}s, Fit {}s, After {}s, Close session {}s, Total {}s".format(buffer_count,start_fit-start_transition,end_fit-start_fit,end_transition-end_fit,end_closing_session - start_closing_session, end_transition-start_transition))

return model_string

$$
LANGUAGE plpythonu
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `NO SQL', `');


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.cnn_keras_merge(
    state1          BYTEA,
    state2          BYTEA
) RETURNS BYTEA AS
$$
import numpy as np
import time
#import plpy
#merge_start = time.time()
if not state1 or not state2:
    return state1 or state2
model_string = state1 + "modelsplitarvind" + state2
return model_string
$$
LANGUAGE plpythonu
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `NO SQL', `');

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.cnn_keras_final(
    state BYTEA
) RETURNS BYTEA AS
$$
import numpy as np
import gc
gc.collect()
return state
$$
LANGUAGE plpythonu
m4_ifdef(`__HAS_FUNCTION_PROPERTIES__', `NO SQL', `');

DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.cnn_keras_step(REAL[],
                                                      SMALLINT[],
                                                      INTEGER,
                                                      TEXT,
                                                      BYTEA);
CREATE AGGREGATE MADLIB_SCHEMA.cnn_keras_step(
    /* ind_var */             REAL[],
    /* dep_var */             SMALLINT[],
    /* seg */                 INTEGER,
    /* architecture */        TEXT,
    /* previous_state */      BYTEA
)(
    STYPE=BYTEA,
    SFUNC=MADLIB_SCHEMA.cnn_keras_transition,
    PREFUNC=MADLIB_SCHEMA.cnn_keras_merge,
    FINALFUNC=MADLIB_SCHEMA.cnn_keras_final
);
