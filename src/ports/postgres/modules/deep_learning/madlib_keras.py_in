# coding=utf-8
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

import datetime
import numpy as np
import os
import plpy
import sys
import time

# Do not remove `import keras` although it's not directly used in this file.
# For ex if the user passes in the optimizer as keras.optimizers.SGD instead of just
# SGD, then without this import this python file won't find the SGD module
import keras

from keras import backend as K
from keras import utils as keras_utils
from keras.layers import *
from keras.models import *
from keras.optimizers import *
from keras.regularizers import *
import madlib_keras_serializer
from madlib_keras_helper import CLASS_VALUES_COLNAME
from madlib_keras_helper import DEPENDENT_VARTYPE_COLNAME
from madlib_keras_helper import expand_input_dims
from madlib_keras_helper import NORMALIZING_CONST_COLNAME
from madlib_keras_validator import FitInputValidator
from madlib_keras_wrapper import *
from keras_model_arch_table import Format
from keras_model_arch_table import get_model_arch

from utilities.model_arch_info import get_input_shape
from utilities.model_arch_info import get_num_classes
from utilities.utilities import is_platform_pg
from utilities.utilities import madlib_version
from utilities.validate_args import get_col_value_and_type

class fitClass():
    def __init__(schema_madlib, source_table, model, dependent_varname,
        independent_varname, model_arch_table, model_arch_id, compile_params,
        fit_params, num_iterations, use_gpu = True,
        validation_table=None, name="", description="", **kwargs):
        self.schema_madlib = schema_madlib
        self.source_table = source_table
        self.model = model
        self.dependent_varname, = dependent_varname
        self.independent_varname = independent_varname
        self.model_arch_table = model_arch_table
        self.model_arch_id = model_arch_id
        self.compile_params, = compile_params
        self.fit_params = fit_params
        self.num_iterations = num_iterations
        self.use_gpu = bool(use_gpu)
        self.validation_table = validation_table
        self.validation_set_provided = bool(validation_table)
        self.name = name
        self.description = description
        self.compile_params_to_pass = "$madlib$" + compile_params + "$madlib$"
        self.fit_params_to_pass = "$madlib$" + fit_params + "$madlib$"

        self.master_model = model_from_json(model_arch)
        self.model_weights = master_model.get_weights()
        self.input_shape = get_input_shape(model_arch)
        self.num_classes = get_num_classes(model_arch)
        self.model_shapes = get_model_shapes(master_model)
        self.model_arch, self.model_weights_serialized = get_model_arch(model_arch_table, model_arch_id)

        self.validate()
        self.keras_session_stuff_start()
        self.fit()
        self.keras_session_stuff_end()

def db_specific_start():
    if is_platform_pg():
        set_keras_session(use_gpu)
        self.gp_segment_id_col =  -1
    else:
        # Disable GPU on master for gpdb
        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
        self.gp_segment_id_col = 'gp_segment_id'

def db_specific_end():
    if is_platform_pg():
        clear_keras_session()

def validate():
    fit_validator = FitInputValidator(
                source_table, validation_table, model, model_arch_table,
                dependent_varname, independent_varname, num_iterations)
    fit_validator.validate_input_shapes(source_table, input_shape, 2)
    if validation_table:
        fit_validator.validate_input_shapes(validation_table, input_shape, 1)

def initialize_model():
    if model_weights_serialized:
        # If warm start from previously trained model, set weights
        model_weights = madlib_keras_serializer.deserialize_weights_orig(
            model_weights_serialized, model_shapes)
        master_model.set_weights(model_weights)
    self.initial_model_state = madlib_keras_serializer.serialize_weights(
            0, 0, 0, model_weights)

def fit():
    self.start_training_time = datetime.datetime.now()
    self.initialize_model()
    self.train_keras_model()
    end_training_time = datetime.datetime.now()
    self.create_output_tables()

def train_keras_model():
    validation_aggregate_accuracy = []; validation_aggregate_loss = []
    aggregate_loss, aggregate_accuracy, aggregate_runtime = [], [], []

    seg_ids_train, rows_per_seg_train = get_rows_per_seg_from_db(source_table)
    if validation_table:
        seg_ids_val, rows_per_seg_val = get_rows_per_seg_from_db(validation_table)
    run_training_iteration = plpy.prepare("""
        SELECT {schema_madlib}.fit_step(
            {independent_varname}::REAL[],
            {dependent_varname}::SMALLINT[],
            {gp_segment_id_col},
            {num_classes}::INTEGER,
            ARRAY{seg_ids_train},
            ARRAY{rows_per_seg_train},
            $MAD${model_arch}$MAD$::TEXT,
            {compile_params_to_pass}::TEXT,
            {fit_params_to_pass}::TEXT,
            {use_gpu},
            $1
        ) AS iteration_result
        FROM {source_table}
        """.format(**locals()), ["bytea"])
    for i in range(num_iterations):
        start_iteration = time.time()
        iteration_result = plpy.execute(run_training_iteration, [model_state])[0]['iteration_result']
        end_iteration = time.time()
        aggregate_runtime.append(datetime.datetime.now())
        avg_loss, avg_accuracy, model_state = madlib_keras_serializer.deserialize_iteration_state(iteration_result)
        if validation_set_provided:
            _, _, _, updated_weights = madlib_keras_serializer.deserialize_weights(
                model_state, model_shapes)
            master_model.set_weights(updated_weights)
            evaluate_result = get_loss_acc_from_keras_eval(schema_madlib,
                                                           validation_table,
                                                           dependent_varname,
                                                           independent_varname,
                                                           compile_params_to_pass,
                                                           model_arch, model_state,
                                                           use_gpu, seg_ids_val,
                                                           rows_per_seg_val,
                                                           gp_segment_id_col)
            if len(evaluate_result) < 2:
                plpy.error('Calling evaluate on validation data returned < 2 '
                           'metrics. Expected metrics are loss and accuracy')
            validation_loss = evaluate_result[0]
            validation_accuracy = evaluate_result[1]
            validation_aggregate_accuracy.append(validation_accuracy)
            validation_aggregate_loss.append(validation_loss)
        aggregate_loss.append(avg_loss)
        aggregate_accuracy.append(avg_accuracy)

def create_output_tables():
    final_validation_acc = None
    version = madlib_version(schema_madlib)

    if validation_aggregate_accuracy and len(validation_aggregate_accuracy) > 0:
        final_validation_acc = validation_aggregate_accuracy[-1]
    final_validation_loss = None
    if validation_aggregate_loss and len(validation_aggregate_loss) > 0:
        final_validation_loss = validation_aggregate_loss[-1]
    class_values, class_values_type = get_col_value_and_type(
        fit_validator.source_summary_table, CLASS_VALUES_COLNAME)
    norm_const, norm_const_type = get_col_value_and_type(
        fit_validator.source_summary_table, NORMALIZING_CONST_COLNAME)
    dep_vartype = plpy.execute("SELECT {0} AS dep FROM {1}".format(
        DEPENDENT_VARTYPE_COLNAME, fit_validator.source_summary_table))[0]['dep']

    create_output_summary_table = plpy.prepare("""
        CREATE TABLE {0}_summary AS
        SELECT
        $1 AS model_arch_table,
        $2 AS model_arch_id,
        $3 AS model_type,
        $4 AS start_training_time,
        $5 AS end_training_time,
        $6 AS source_table,
        $7 AS validation_table,
        $8 AS model,
        $9 AS dependent_varname,
        $10 AS independent_varname,
        $11 AS name,
        $12 AS description,
        $13 AS model_size,
        $14 AS madlib_version,
        $15 AS compile_params,
        $16 AS fit_params,
        $17 AS num_iterations,
        $18 AS num_classes,
        $19 AS accuracy,
        $20 AS loss,
        $21 AS accuracy_iter,
        $22 AS loss_iter,
        $23 AS time_iter,
        $24 AS accuracy_validation,
        $25 AS loss_validation,
        $26 AS accuracy_iter_validation,
        $27 AS loss_iter_validation,
        $28 AS {1},
        $29 AS {2},
        $30 AS {3}
        """.format(model, CLASS_VALUES_COLNAME, DEPENDENT_VARTYPE_COLNAME,
                   NORMALIZING_CONST_COLNAME),
                   ["TEXT", "INTEGER", "TEXT", "TIMESTAMP",
                    "TIMESTAMP", "TEXT", "TEXT","TEXT",
                    "TEXT", "TEXT", "TEXT", "TEXT", "INTEGER",
                    "TEXT", "TEXT", "TEXT", "INTEGER",
                    "INTEGER", "DOUBLE PRECISION",
                    "DOUBLE PRECISION", "DOUBLE PRECISION[]",
                    "DOUBLE PRECISION[]", "TIMESTAMP[]",
                    "DOUBLE PRECISION", "DOUBLE PRECISION",
                    "DOUBLE PRECISION[]", "DOUBLE PRECISION[]",
                    class_values_type, "TEXT", norm_const_type])
    plpy.execute(
        create_output_summary_table,
        [
            model_arch_table, model_arch_id,
            "madlib_keras",
            start_training_time, end_training_time,
            source_table, validation_table,
            model, dependent_varname,
            independent_varname, name, description,
            sys.getsizeof(model), version, compile_params,
            fit_params, num_iterations, num_classes,
            aggregate_accuracy[-1],
            aggregate_loss[-1],
            aggregate_accuracy, aggregate_loss,
            aggregate_runtime, final_validation_acc,
            final_validation_loss,
            validation_aggregate_accuracy,
            validation_aggregate_loss,
            class_values,
            dep_vartype,
            norm_const
        ]
        )

    create_output_table = plpy.prepare("""
        CREATE TABLE {0} AS
        SELECT $1 as model_data""".format(model), ["bytea"])
    plpy.execute(create_output_table, [model_state])
def get_rows_per_seg_from_db(table_name):
    """
    This function queries the given table and returns the total rows per segment.
    Since we cannot pass a dictionary to the keras fit step function we create arrays
    out of the segment numbers and the rows per segment values.
    This function assumes that the table is not empty.
    :param table_name:
    :return: Returns two arrays
    1. An array containing all the segment numbers in ascending order
    1. An array containing the total rows for each of the segments in the
    segment array
    """
    if is_platform_pg():
        rows = plpy.execute(
            """ SELECT count(*) AS rows_per_seg
                FROM {0}
            """.format(table_name))
        seg_ids = "[]::integer[]"
    else:
        # Compute total buffers on each segment
        rows = plpy.execute(
            """ SELECT gp_segment_id, count(*) AS rows_per_seg
                FROM {0}
                GROUP BY gp_segment_id
            """.format(table_name))
        seg_ids = [int(row["gp_segment_id"]) for row in rows]

    rows = [int(row["rows_per_seg"]) for row in rows]
    return seg_ids, rows


def fit_transition(state, ind_var, dep_var, current_seg_id, num_classes,
                   all_seg_ids, total_buffers_per_seg, architecture,
                   compile_params, fit_params, use_gpu, previous_state,
                   **kwargs):

    """

    :param state:
    :param ind_var:
    :param dep_var:
    :param current_seg_id:
    :param num_classes:
    :param all_seg_ids:
    :param total_buffers_per_seg:
    :param architecture:
    :param compile_params:
    :param fit_params:
    :param use_gpu:
    :param previous_state:
    :param kwargs:
    :return:
    """
    if not ind_var or not dep_var:
        return state

    start_transition = time.time()
    SD = kwargs['SD']
    # Configure GPUs/CPUs
    device_name = get_device_name_and_set_cuda_env(use_gpu, current_seg_id)

    # Set up system if this is the first buffer on segment'

    if not state:
        if not is_platform_pg():
            set_keras_session(use_gpu)
        segment_model = model_from_json(architecture)
        SD['model_shapes'] = madlib_keras_serializer.get_model_shapes(segment_model)
        compile_and_set_weights(segment_model, compile_params, device_name,
                                previous_state, SD['model_shapes'])
        SD['segment_model'] = segment_model
        SD['buffer_count'] = 0
        agg_loss = 0
        agg_accuracy = 0
    else:
        segment_model = SD['segment_model']
        #TODO we don't need to deserialize the weights here.
        agg_loss, agg_accuracy, _, _ = madlib_keras_serializer.deserialize_weights(
            state, SD['model_shapes'])

    # Prepare the data
    x_train = np.array(ind_var, dtype='float64')
    y_train = np.array(dep_var)

    # Fit segment model on data
    start_fit = time.time()
    with K.tf.device(device_name):
        #TODO consider not doing this every time
        fit_params = parse_fit_params(fit_params)
        history = segment_model.fit(x_train, y_train, **fit_params)
        loss = history.history['loss'][0]
        accuracy = history.history['acc'][0]
    end_fit = time.time()

    # Re-serialize the weights
    # Update buffer count, check if we are done
    SD['buffer_count'] += 1
    agg_loss += loss
    agg_accuracy += accuracy

    with K.tf.device(device_name):
        updated_weights = segment_model.get_weights()
    if is_platform_pg():
        total_buffers_per_seg = total_buffers_per_seg[0]
    else:
        total_buffers_per_seg = total_buffers_per_seg[all_seg_ids.index(current_seg_id)]
    if total_buffers_per_seg == 0:
        plpy.error('total buffers is 0')

    if SD['buffer_count'] == total_buffers_per_seg:
        agg_loss /= total_buffers_per_seg
        agg_accuracy /= total_buffers_per_seg
        if not is_platform_pg():
            # In GPDB, each segment would have a keras session, so clear
            # them after the last buffer is processed.
            clear_keras_session()

    new_model_state = madlib_keras_serializer.serialize_weights(
        agg_loss, agg_accuracy, SD['buffer_count'], updated_weights)

    del x_train
    del y_train

    end_transition = time.time()
    plpy.info("Processed buffer {0}: Fit took {1} sec, Total was {2} sec".format(
        SD['buffer_count'], end_fit - start_fit, end_transition - start_transition))

    return new_model_state

def fit_merge(state1, state2, **kwargs):
    # Return if called early
    if not state1 or not state2:
        return state1 or state2

    # Deserialize states
    loss1, accuracy1, buffer_count1, weights1 = madlib_keras_serializer.deserialize_weights_merge(state1)
    loss2, accuracy2, buffer_count2, weights2 = madlib_keras_serializer.deserialize_weights_merge(state2)
        # plpy.info('merge buffer loss1 {}, accuracy1 {}, buffer count1 {}'.format(loss1, accuracy1, buffer_count1))
    # plpy.info('merge buffer loss2 {}, accuracy2 {}, buffer count2 {}'.format(loss2, accuracy2, buffer_count2))

    # Compute total buffer counts
    # buffer_count1, buffer_count2 = state1[2], state2[2]
    total_buffers = (buffer_count1 + buffer_count2) * 1.0
    if total_buffers == 0:
        plpy.error('total buffers in merge is 0')
    merge_weight1 = buffer_count1 / total_buffers
    merge_weight2 = buffer_count2 / total_buffers

    # Average the losses
    # loss1, loss2 = state1[0], state2[0]
    avg_loss = merge_weight1*loss1 + merge_weight2*loss2

    # Average the accuracies
    # accuracy1, accuracy2 = state1[1], state2[1]
    avg_accuracy = merge_weight1*accuracy1 + merge_weight2*accuracy2

    # Average the weights
    # weights1, weights2 = state1[3:], state2[3:]
    avg_weights = merge_weight1*weights1 + merge_weight2*weights2
    # avg_weights = [(merge_weight1 * e1) + (merge_weight2 * e2) for e1, e2 in zip(weights1, weights2)]

    # Return the merged state
    return madlib_keras_serializer.serialize_weights_merge(
        avg_loss, avg_accuracy, total_buffers, avg_weights)

def fit_final(state, **kwargs):
    return state

def evaluate1(schema_madlib, model_table, test_table, id_col, model_arch_table,
            model_arch_id, dependent_varname, independent_varname,
            compile_params, output_table, **kwargs):
    # module_name = 'madlib_keras_evaluate'
    # input_tbl_valid(test_table, module_name)
    # input_tbl_valid(model_arch_table, module_name)
    # output_tbl_valid(output_table, module_name)

    # _validate_input_args(test_table, model_arch_table, output_table)

    model_data_query = "SELECT model_data from {0}".format(model_table)
    model_data = plpy.execute(model_data_query)[0]['model_data']

    model_arch_query = "SELECT model_arch, model_weights FROM {0} " \
                       "WHERE id = {1}".format(model_arch_table, model_arch_id)
    query_result = plpy.execute(model_arch_query)
    if not  query_result or len(query_result) == 0:
        plpy.error("no model arch found in table {0} with id {1}".format(
            model_arch_table, model_arch_id))
    query_result = query_result[0]
    model_arch = query_result[Format.MODEL_ARCH]
    compile_params = "$madlib$" + compile_params + "$madlib$"

    loss_acc = get_loss_acc_from_keras_eval(schema_madlib, test_table, dependent_varname,
                                            independent_varname, compile_params, model_arch,
                                            model_data, False)
    #TODO remove these infos after adding create table command
    plpy.info('len of evaluate result is {}'.format(len(loss_acc)))
    plpy.info('evaluate result loss is {}'.format(loss_acc[0]))
    plpy.info('evaluate result acc is {}'.format(loss_acc[1]))

def get_loss_acc_from_keras_eval(schema_madlib, table, dependent_varname,
                                 independent_varname, compile_params, model_arch,
                                 model_data, use_gpu, seg_ids_val,
                                 rows_per_seg_val, gp_segment_id_col):
    """
    This function will call the internal keras evaluate function to get the loss
    and accuracy of each tuple which then gets averaged to get the final result.
    """
    evaluate_query = plpy.prepare("""
    select {schema_madlib}.array_avg(loss_acc, True) as final_loss_acc from
    (
        select ({schema_madlib}.internal_keras_evaluate({dependent_varname},
                                            {independent_varname},
                                            $MAD${model_arch}$MAD$,
                                            $1, {compile_params},
                                            {use_gpu}, 
                                            ARRAY{seg_ids_val}, 
                                            ARRAY{rows_per_seg_val},
                                            {gp_segment_id_col})) as loss_acc 
        from {table}
    ) q""".format(**locals()), ["bytea"])
    res = plpy.execute(evaluate_query, [model_data])
    loss_acc = res[0]['final_loss_acc']
    return loss_acc


def internal_keras_evaluate(dependent_var, independent_var, model_architecture,
                            model_data, compile_params, use_gpu, seg_ids_val,
                            rows_per_seg_val, current_seg, **kwargs):
    SD = kwargs['SD']
    device_name = get_device_name_and_set_cuda_env(use_gpu, current_seg)

    if 'segment_model' not in SD:
        if not is_platform_pg():
            set_keras_session(use_gpu)
        model = model_from_json(model_architecture)
        model_shapes = madlib_keras_serializer.get_model_shapes(model)
        _, _, _, model_weights = madlib_keras_serializer.deserialize_weights(
            model_data, model_shapes)
        model.set_weights(model_weights)
        with K.tf.device(device_name):
            compile_model(model, compile_params)
        SD['segment_model'] = model
        SD['row_count'] = 0
    else:
        model = SD['segment_model']
    SD['row_count'] += 1

    # Since the training data is batched but the validation data isn't,
    # we have to make sure that the validation data np array has the same
    # number of dimensions as training data. So we prepend a dimension to
    # both x and y np arrays using expand_dims.
    independent_var = expand_input_dims(independent_var, target_type='float32')
    dependent_var = expand_input_dims(dependent_var)


    with K.tf.device(device_name):
        res = model.evaluate(independent_var, dependent_var)
    if current_seg == -1:
        total_rows = rows_per_seg_val[0]
    else:
        total_rows = rows_per_seg_val[seg_ids_val.index(current_seg)]

    if is_last_row_in_seg(SD['row_count'], total_rows):
        SD.pop('segment_model', None)
        if not is_platform_pg():
            clear_keras_session()

    return res


def is_last_row_in_seg(row_count, total_rows):
    return row_count == total_rows
