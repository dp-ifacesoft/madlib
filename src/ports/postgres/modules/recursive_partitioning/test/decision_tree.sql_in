---------------------------------------------------------------------------
-- training
DROP TABLE IF EXISTS dummy_dt_con_src CASCADE;
CREATE TABLE dummy_dt_con_src (
    id  integer,
    cat integer[],
    con float8[],
    y   float8
);

INSERT INTO dummy_dt_con_src VALUES
(1, '{0}'::integer[], ARRAY[0], 0.5),
(2, '{0}'::integer[], ARRAY[1], 0.5),
(3, '{0}'::integer[], ARRAY[4], 0.5),
(4, '{0}'::integer[], ARRAY[5], 0.1),
(5, '{0}'::integer[], ARRAY[6], 0.1),
(6, '{1}'::integer[], ARRAY[9], 0.1);

------------------------------------------------------------
-- entropy
SELECT
    assert(relative_error(_dst_compute_entropy(i, 10), 1.0) < 1e-6,
           'expect 1 bit entropy, got ' || _dst_compute_entropy(i, 10)::text)
FROM
    (SELECT generate_series(0, 1) AS i) seq
;

------------------------------------------------------------
-- con_splits
DROP TABLE IF EXISTS dummy_splits;
CREATE TABLE dummy_splits AS
SELECT _dst_compute_con_splits(
    con,
    9,
    5::smallint
) as splits
FROM dummy_dt_con_src
;

SELECT * FROM dummy_splits;

---------------------------------------------------------------------------
-- cat encoding
SELECT
    assert(
        relative_error(_map_catlevel_to_int('{B}', '{A,B}', ARRAY[2], TRUE), ARRAY[1]) < 1e-6,
        'wrong results in _map_catlevel_to_int()')
;

------------------------------------------------------------
-- test training aggregate manually
\x on
SELECT _print_decision_tree(_initialize_decision_tree(TRUE, 'mse', 1::smallint, 5::smallint));

-------------------------------------------------------------------------
-- regression tree for single level
SELECT
    *
    -- , assert(tree_depth IS NOT NULL, 'dummy_dt (empty tree returned)'),
    -- assert(tree_depth = 1, 'dummy_dt (wrong tree_depth)'),
    -- assert(relative_error(feature_indices, ARRAY[0,-1,-1]) = 0.,
    --        'dummy_dt (wrong feature_indices)'),
    -- assert(feature_thresholds[1] = 4, 'dummy_dt (wrong feature_thresholds)'),
    -- assert(is_categorical[1] = 0, 'dummy_dt (wrong is_categorical)'),
    -- assert(relative_error(predictions[1:1][2:3], ARRAY[0.5, 0.1]) < 1e-6,
    --        'dummy_dt (wrong predictions)')
FROM (
    SELECT (tree).*
    FROM (
        SELECT
            _print_decision_tree(
                (_dt_apply(
                    _initialize_decision_tree(TRUE, 'mse', 1::smallint, 5::smallint),
                    _compute_leaf_stats(
                        _initialize_decision_tree(TRUE, 'mse', 1::smallint, 5::smallint),
                        cat::integer[],
                        con::double precision[],
                        y::double precision,
                        1.0::double precision,
                        '{2}'::integer[],
                        (SELECT splits FROM dummy_splits)::BYTEA8,
                        2::smallint,
                        FALSE),
                    (SELECT splits FROM dummy_splits),
                    2::smallint,
                    1::smallint,
                    10::smallint,
                    False::boolean,
                    1::integer
                )).tree_state
            ) as tree
        FROM dummy_dt_con_src
    ) q1
) q2
;

-------------------------------------------------------------------------
-- classification tree for multi-levels
DROP TABLE IF EXISTS dummy_dt_cat_src CASCADE;
CREATE TABLE dummy_dt_cat_src (
    id  integer,
    cat integer[],
    con float8[],
    y   integer,
    weight float8
);

INSERT INTO dummy_dt_cat_src VALUES
(1,  '{0, 0}'::integer[], ARRAY[100,     0],    0,  10),
(2,  '{0, 0}'::integer[], ARRAY[200,     1],    0,  10),
(3,  '{0, 1}'::integer[], ARRAY[1000,    4],    0,  10),
(4,  '{0, 1}'::integer[], ARRAY[1000,    4],    0,  1),
(5,  '{0, 1}'::integer[], ARRAY[1000,    4],    0,  1),
(6,  '{0, 1}'::integer[], ARRAY[10,      5],    1,  1),
(7,  '{0, 2}'::integer[], ARRAY[10.10,   10],   1,  1),
(8,  '{1, 2}'::integer[], ARRAY[12,      9],    1,  1),
(8,  '{1, 2}'::integer[], ARRAY[13.4,    9],    1,  1),
(8,  '{1, 2}'::integer[], ARRAY[15,      9],    1,  1),
(9,  '{1, 2}'::integer[], ARRAY[15.4,    9],    1,  0.9),
(10, '{1, 2}'::integer[], ARRAY[5.4,     10],   1,  0.9),
(11, '{1, 2}'::integer[], ARRAY[15.4,    10],   1,  0.9),
(11, '{1, 2}'::integer[], ARRAY[25.4,    1],    1,  0.9),
(12, '{1, 2}'::integer[], ARRAY[15.4,    -20],  1,  0.9),
(13, '{1, 2}'::integer[], ARRAY[15.4,    200],  1,  0.9),
(14, '{1, 2}'::integer[], ARRAY[15.4,    0],    1,  0.9),
(15, '{1, 2}'::integer[], ARRAY[11.4,    1],    1,  0.9),
(16, '{1, 2}'::integer[], ARRAY[124,     2],    1,  0.9);

DROP TABLE IF EXISTS dummy_splits;
CREATE TABLE dummy_splits AS
SELECT _dst_compute_con_splits(
    con,
    9,
    3::smallint
) as splits
FROM dummy_dt_cat_src
;

-- Add first iteration of tree in an output table
create table dummy_dt_output as
    SELECT
        1::integer as iteration,
        (_dt_apply(
            _initialize_decision_tree(FALSE, 'entropy', 2::smallint, 5::smallint),
            _compute_leaf_stats(
                _initialize_decision_tree(FALSE, 'entropy', 2::smallint, 5::smallint),
                cat,
                con,
                y,
                weight::float8,
                '{2, 3}'::integer[],
                (SELECT splits FROM dummy_splits),
                2::smallint,
                FALSE),
            (SELECT splits FROM dummy_splits),
            2::smallint,
            0::smallint,
            3::smallint,
            False::boolean,
            1::integer
        )).tree_state as tree
        FROM dummy_dt_cat_src
;

SELECT * FROM dummy_dt_output;

-- Train tree further
insert into dummy_dt_output
SELECT
    2::integer as iteration,
    (_dt_apply(
        (SELECT tree from dummy_dt_output where iteration = 1),
        _compute_leaf_stats(
            (SELECT tree from dummy_dt_output where iteration = 1),
            cat,
            con,
            y,
            1.0::float8,
            '{2, 3}'::integer[],
            (SELECT splits FROM dummy_splits),
            2::smallint,
            FALSE),
        (SELECT splits FROM dummy_splits),
        2::smallint,
        1::smallint,
        3::smallint,
        False::boolean,
        1::integer
    )).tree_state as tree
    FROM dummy_dt_cat_src
;

-- Final iteration to wrap the training of tree
-- (this iteration ends training but does not expand tree further)
SELECT
    *
    -- , assert(tree_depth IS NOT NULL, 'dummy_dt (empty tree returned)'),
    -- assert(tree_depth = 2, 'dummy_dt (wrong tree_depth)'),
    -- assert(relative_error(feature_indices, ARRAY[1,0,-2,-2,-2,-3,-3]) = 0.,
    --        'dummy_dt (wrong feature_indices)'),
    -- assert(relative_error(feature_thresholds, ARRAY[1,12,0,0,0,0,0]) = 0.,
    --        'dummy_dt (wrong feature_thresholds)'),
    -- assert(relative_error(is_categorical, ARRAY[1,0,0,0,0,0,0]) = 0.,
    --        'dummy_dt (wrong is_categorical)')
FROM (
        SELECT (tree).*
        FROM (
            SELECT
                _print_decision_tree(
                    (_dt_apply(
                        (SELECT tree from dummy_dt_output where iteration = 2),
                        _compute_leaf_stats(
                            (SELECT tree from dummy_dt_output where iteration = 2),
                            cat,
                            con,
                            y,
                            1.0::float8,
                            '{2, 3}'::integer[],
                            (SELECT splits FROM dummy_splits),
                            2::smallint,
                            FALSE),
                        (SELECT splits FROM dummy_splits),
                        2::smallint,
                        1::smallint,
                        3::smallint,
                        False::boolean,
                        1::integer
                    )).tree_state
                ) as tree
                FROM dummy_dt_cat_src
        ) q1
) q2
;
--------------------------------------------------------------------------------
-- Validate tree train function ------------------------------------------------
DROP TABLE IF EXISTS dt_golf CASCADE;
CREATE TABLE dt_golf (
    id integer NOT NULL,
    id_2 integer,
    "OUTLOOK" text,
    temperature double precision,
    humidity double precision,
    "Cont_features" double precision[],
    cat_features text[],
    windy boolean,
    windy2 boolean,
    class text
) ;

INSERT INTO dt_golf (id,"OUTLOOK",temperature,humidity,"Cont_features",cat_features, windy, windy2,class) VALUES
(1, 'sunny', 85, 85,ARRAY[85, 85], ARRAY['a', 'b'], false, false, 'Don''t Play'),
(2, 'sunny', 80, 90, ARRAY[80, 90], ARRAY['a', 'b'], true, false, 'Don''t Play'),
(3, 'overcast', 83, 78, ARRAY[83, 78], ARRAY['a', 'b'], false, false, 'Play'),
(4, 'rain', 70, NULL, ARRAY[70, 96], ARRAY['a', 'b'], false, false, 'Play'),
(5, 'rain', 68, 80, ARRAY[68, 80], ARRAY['a', 'b'], false, false, 'Play'),
(6, 'rain', NULL, 70, ARRAY[65, 70], ARRAY['a', 'b'], true, false, 'Don''t Play'),
(7, 'overcast', 64, 65, ARRAY[64, 65], ARRAY['c', 'b'], NULL, NULL, 'Play'),
(8, 'sunny', 72, 95, ARRAY[72, 95], ARRAY['a', 'b'], false, false, 'Don''t Play'),
(9, 'sunny', 69, 70, ARRAY[69, 70], ARRAY['a', 'b'], false, false, 'Play'),
(10, 'rain', 75, 80, ARRAY[75, 80], ARRAY['a', 'b'], false, false, 'Play'),
(11, 'sunny', 75, 70, ARRAY[75, 70], ARRAY['a', 'd'], true, false, 'Play'),
(12, 'overcast', 72, 90, ARRAY[72, 90], ARRAY['c', 'b'], NULL, NULL, 'Play'),
(13, 'overcast', 81, 75, ARRAY[81, 75], ARRAY['a', 'b'], false, false, 'Play'),
(15, NULL, 81, 75, ARRAY[81, 75], ARRAY['a', 'b'], false, false, 'Play'),
(16, 'overcast', NULL, 75, ARRAY[81, 75], ARRAY['a', 'd'], false, false, 'Play'),
(14, 'rain', 71, 80, ARRAY[71, 80], ARRAY['c', 'b'], true, false, 'Don''t Play');

update dt_golf set id_2 = id % 2;
-------------------------------------------------------------------------

-- no grouping, with cross_validation
-- also adding a categorical with just a single level (windy2)
DROP TABLE IF EXISTS train_output, train_output_summary, train_output_cv;
SELECT tree_train('dt_golf'::text,         -- source table
                         'train_output'::text,    -- output model table
                         'id'::text,              -- id column
                         'temperature::double precision'::text,           -- response
                         'humidity, windy2, "Cont_features"'::text,   -- features
                         NULL::text,        -- exclude columns
                         'gini'::text,      -- split criterion
                         NULL::text,        -- no grouping
                         NULL::text,        -- no weights
                         10::integer,       -- max depth
                         6::integer,        -- min split
                         2::integer,        -- min bucket
                         3::integer,        -- number of bins per continuous variable
                         'cp=0.01, n_folds=2',          -- cost-complexity pruning parameter
                         'null_as_category=True'
                         );
SELECT _print_decision_tree(model_data) from train_output;
SELECT tree_display('train_output', False);
SELECT impurity_var_importance FROM train_output;
SELECT * FROM train_output_cv;
SELECT * FROM train_output_summary;
-------------------------------------------------------------------------

-- grouping
DROP TABLE IF EXISTS train_output, train_output_summary, predict_output;
SELECT tree_train('dt_golf'::text,         -- source table
                  'train_output'::text,    -- output model table
                  'id'::text,              -- id column
                  'temperature::double precision'::text,       -- response
                  '"OUTLOOK", humidity, windy, cat_features'::text, -- features
                  NULL::text,        -- exclude columns
                  'gini'::text,      -- split criterion
                  'class'::text,     -- grouping
                  NULL::text,        -- no weights
                  10::integer,       -- max depth
                  2::integer,        -- min split
                  1::integer,        -- min bucket
                  3::integer,        -- number of bins per contvariable
                  'cp=0',          -- cost-complexity pruning parameter
                  'max_surrogates=2'
                  );
\d train_output;
SELECT _print_decision_tree(model_data) from train_output;
SELECT tree_display('train_output', FALSE);

-- cat_features[2] has a single level. The cat_n_levels is in order of the
-- input categorical features.
-- In this case "OUTLOOL" = 1, windy = 2, cat_features[1] = 3, cat_features[2] = 4
SELECT assert(cat_n_levels[4] = 1,
              'Categorical features with single level not being retained.')
FROM train_output
WHERE class = E'Don\'t Play';

-- testing tree_predict with a category not present in training table
CREATE TABLE dt_golf2 as
SELECT * FROM dt_golf
UNION
SELECT 15 as id, 1 as id_2, 'humid' as "OUTLOOK", 71 as temperature, 80 as humidity,
       ARRAY[90, 90] as "Cont_features", ARRAY['b', 'c'] as cat_features,
       true as windy, false as windy2, 'Don''t Play' as class;
\x off
SELECT * FROM dt_golf2;
SELECT tree_predict('train_output', 'dt_golf2', 'predict_output');
SELECT *
FROM
    predict_output
JOIN
    dt_golf2
USING (id);
\x on
select * from train_output_summary;
-------------------------------------------------------------------------

-- grouping and null_as_category=True
DROP TABLE IF EXISTS train_output, train_output_summary, predict_output;
SELECT tree_train('dt_golf'::text,         -- source table
                  'train_output'::text,    -- output model table
                  'id'::text,              -- id column
                  'temperature::double precision'::text,           -- response
                  '"OUTLOOK", humidity'::text,   -- features
                  NULL::text,        -- exclude columns
                  'mse'::text,       -- split criterion
                  'id_2'::text,      -- grouping col
                  NULL::text,        -- no weights
                  NULL::integer,     -- max depth
                  6::integer,        -- min split
                  2::integer,        -- min bucket
                  3::integer,        -- number of bins per continuous variable
                  'cp=0.01',
                  'max_surrogates=2, null_as_category=True'
                  );

SELECT _print_decision_tree(model_data) from train_output;
SELECT tree_display('train_output', False);
SELECT tree_surr_display('train_output');
SELECT * FROM train_output;
SELECT tree_predict('train_output', 'dt_golf', 'predict_output');
\x off
SELECT *
FROM
    predict_output
JOIN
    dt_golf
USING (id);
\x on
select * from train_output;
select * from train_output_summary;
-------------------------------------------------------------------------

-- variable importance check
DROP TABLE IF EXISTS train_output, train_output_summary, predict_output;
SELECT tree_train('dt_golf'::text,         -- source table
                  'train_output'::text,    -- output model table
                  'id'::text,              -- id column
                  'temperature::double precision'::text,           -- response
                  '"OUTLOOK", temperature'::text,   -- features
                  NULL::text,        -- exclude columns
                  'mse'::text,       -- split criterion
                  NULL::text,      -- grouping col
                  NULL::text,        -- no weights
                  NULL::integer,     -- max depth
                  6::integer,        -- min split
                  2::integer,        -- min bucket
                  3::integer,        -- number of bins per continuous variable
                  'cp=0.01',
                  ''
                  );

SELECT impurity_var_importance FROM train_output;
SELECT assert(impurity_var_importance[1] < 1,
              'Variable importance not valid for extreme case')
FROM train_output;
-------------------------------------------------------------------------

drop table if exists group_cp;
create table group_cp(class TEXT,
                      explore_value  DOUBLE PRECISION);
insert into group_cp values
    ('Don''t Play', 0.5),
    ('Play', -0.1);

-- Test if __build_tree works with an array of cp values
-- this should be uncommented after __build_tree has been changed to accept
-- an array of cp values instead of a single cp value
drop table if exists train_output, train_output_summary;
select __build_tree(
    FALSE,
    'mse',
    'dt_golf',
    'train_output',
    'id',
    'temperature::double precision',
    FALSE,
    '"OUTLOOK"',
    ARRAY['"OUTLOOK"']::text[],
    '{}',
    '{}',
    '{humidity}',
    'class',
    '1',
    4,
    2,
    1,
    3,
    'group_cp',
    0::smallint,
    'notice',
    NULL,
    0
    );

select tree_display('train_output', FALSE);
select * from train_output;
select * from train_output_summary;

-------------------------------------------------------------------------
CREATE TABLE array_test AS
SELECT i as id, array_fill(array_of_float(100), random() * i) as feat, i as dep
FROM generate_series(1, 10) as i;


DROP TABLE IF EXISTS train_output, train_output_summary, train_output_cv, predict_output;
SELECT tree_train('array_test'::text,         -- source table
                  'train_output'::text,    -- output model table
                  'id'::text,              -- id column
                  'dep::double precision'::text,           -- response
                  'feat'::text,   -- features
                  NULL::text,        -- exclude columns
                  'mse'::text,      -- split criterion
                  NULL::text,        -- no grouping
                  NULL::text,        -- no weights
                  2::integer,     -- max depth
                  6::integer,        -- min split
                  2::integer,        -- min bucket
                  8::integer        -- number of bins per continuous variable
              );
SELECT * FROM train_output_summary;
SELECT _print_decision_tree(model_data) FROM train_output;
SELECT tree_display('train_output', False);
SELECT tree_predict('train_output', 'array_test', 'predict_output');


-- Test get var importance function
DROP TABLE IF EXISTS train_output, train_output_summary, train_output_cv, predict_output;
SELECT tree_train('dt_golf',         -- source table
                   'train_output',    -- output model table
                   'id',              -- id column
                   'class',           -- response
                   '"OUTLOOK", temperature',   -- features
                   NULL::text,        -- exclude columns
                   'gini',            -- split criterion
                   NULL::text,        -- no grouping
                   NULL::text,        -- no weights, all observations treated equally
                   5,                 -- max depth
                   3,                 -- min split
                   1,                 -- min bucket
                   10                 -- number of bins per continuous variable
                   );
DROP TABLE IF EXISTS var_output;
SELECT get_var_importance('train_output', 'var_output');

SELECT * FROM train_output;
SELECT * from var_output;

------------------------------------------------------------------
-- Regression Tests
------------------------------------------------------------------
DROP TABLE IF EXISTS mt_cars;
CREATE TABLE mt_cars (
    id integer NOT NULL,
    mpg double precision,
    cyl integer,
    disp double precision,
    hp integer,
    drat double precision,
    wt double precision,
    qsec double precision,
    vs integer,
    am integer,
    gear integer,
    carb integer
);
INSERT INTO mt_cars VALUES
(1,18.7,8,360,175,3.15,3.44,17.02,0,0,3,2),
(2,21,6,160,110,3.9,2.62,16.46,0,1,4,4),
(3,24.4,4,146.7,62,3.69,3.19,20,1,0,4,2),
(4,21,6,160,110,3.9,2.875,17.02,0,1,4,4),
(5,17.8,6,167.6,123,3.92,3.44,18.9,1,0,4,4),
(6,16.4,8,275.8,180,3.078,4.07,17.4,0,0,3,3),
(7,22.8,4,108,93,3.85,2.32,18.61,1,1,4,1),
(8,17.3,8,275.8,180,3.078,3.73,17.6,0,0,3,3),
(9,21.4,null,258,110,3.08,3.215,19.44,1,0,3,1),
(10,15.2,8,275.8,180,3.078,3.78,18,0,0,3,3),
(11,18.1,6,225,105,2.768,3.46,20.22,1,0,3,1),
(12,32.4,4,78.7,66,4.08,2.20,19.47,1,1,4,1),
(13,14.3,8,360,245,3.21,3.578,15.84,0,0,3,4),
(14,22.8,4,140.8,95,3.92,3.15,22.9,1,0,4,2),
(15,30.4,4,75.7,52,4.93,1.615,18.52,1,1,4,2),
(16,19.2,6,167.6,123,3.92,3.44,18.3,1,0,4,4),
(17,33.9,4,71.14,65,4.22,1.835,19.9,1,1,4,1),
(18,15.2,null,304,150,3.15,3.435,17.3,0,0,3,2),
(19,10.4,8,472,205,2.93,5.25,17.98,0,0,3,4),
(20,27.3,4,79,66,4.08,1.935,18.9,1,1,4,1),
(21,10.4,8,460,215,3,5.424,17.82,0,0,3,4),
(22,26,4,120.3,91,4.43,2.14,16.7,0,1,5,2),
(23,14.7,8,440,230,3.23,5.345,17.42,0,0,3,4),
(24,30.4,4,95.14,113,3.77,1.513,16.9,1,1,5,2),
(25,21.5,4,120.1,97,3.70,2.465,20.01,1,0,3,1),
(26,15.8,8,351,264,4.22,3.17,14.5,0,1,5,4),
(27,15.5,8,318,150,2.768,3.52,16.87,0,0,3,2),
(28,15,8,301,335,3.54,3.578,14.6,0,1,5,8),
(29,13.3,8,350,245,3.73,3.84,15.41,0,0,3,4),
(30,19.2,8,400,175,3.08,3.845,17.05,0,0,3,2),
(31,19.7,6,145,175,3.62,2.77,15.5,0,1,5,6),
(32,21.4,4,121,109,4.11,2.78,18.6,1,1,4,2);

DROP TABLE IF EXISTS train_output, train_output_summary, train_output_cv;
SELECT madlib.tree_train('mt_cars',         -- source table
                         'train_output',    -- output model table
                         'id',              -- id column
                         'mpg',             -- dependent variable
                         '*',               -- features
                         'id, hp, drat, am, gear, carb',  -- exclude columns
                         'mse',             -- split criterion
                         NULL::text,        -- no grouping
                         NULL::text,        -- no weights, all observations treated equally
                         10,                -- max depth
                         8,                 -- min split
                         3,                 -- number of bins per continuous variable
                         10,                -- number of splits
                         NULL,              -- pruning parameters
                         'max_surrogates=2' -- number of surrogates
                         );

DROP TABLE IF EXISTS imp_output;
SELECT madlib.get_var_importance('train_output','imp_output');
SELECT * FROM imp_output ORDER BY impurity_var_importance DESC;

DROP TABLE IF EXISTS prediction_results;
SELECT madlib.tree_predict('train_output',
                           'mt_cars',
                           'prediction_results',
                           'response');
SELECT s.id, mpg, estimated_mpg, mpg-estimated_mpg as delta
FROM prediction_results p,
mt_cars s WHERE s.id = p.id ORDER BY id;

SELECT madlib.tree_display('train_output', FALSE);

SELECT madlib.tree_surr_display('train_output');


DROP TABLE IF EXISTS train_output, train_output_summary;
SELECT madlib.tree_train('dt_golf',
'train_output',
'id',
'class',
'"OUTLOOK", temperature, windy',
NULL::text,
'gini',
NULL::text,
NULL::text,
5,
3,
1,
10);

DROP TABLE IF EXISTS prediction_results;
SELECT madlib.tree_predict('train_output', 'dt_golf', 'prediction_results', 'prob');
------------------------------------------------------------------
-- Test for Model versioning
------------------------------------------------------------------
-- Call tree_train multiple times without dropping the output table.
-- Assert that it creates the view and the repo table along with the summary table
SELECT madlib.tree_train('mt_cars',         -- source table
                         'train_view',    -- output model table
                         'id',              -- id column
                         'mpg',             -- dependent variable
                         '*',               -- features
                         'id, hp, drat, am, gear, carb',  -- exclude columns
                         'mse',             -- split criterion
                         NULL::text,        -- no grouping
                         NULL::text,        -- no weights, all observations treated equally
                         10,                -- max depth
                         8,                 -- min split
                         3,                 -- number of bins per continuous variable
                         10,                -- number of splits
                         NULL,              -- pruning parameters
                         'max_surrogates=2',-- number of surrogates
                         NULL,'train_repo','model 1','desc 1');

SELECT assert(max_model_id = 1 , 'The highest model_id should have been 1.')
FROM (SELECT max(model_id) AS max_model_id FROM train_repo) s;

-- predict using the view.
DROP TABLE IF EXISTS predict_output;
SELECT tree_predict('train_view', 'mt_cars', 'predict_output');

SELECT tree_train('dt_golf',         -- source table
                   'train_view',    -- output model table
                   'id',              -- id column
                   'class',           -- response
                   '"OUTLOOK", temperature',   -- features
                   NULL,        -- exclude columns
                   'gini',            -- split criterion
                   NULL,            -- grouping
                   NULL,        -- no weights, all observations treated equally
                   NULL,                 -- max depth
                   6,                 -- min split
                   2,                 -- min bucket
                   3,                 -- number of bins per continuous variable
                   NULL,NULL,NULL,'train_repo','name 2','desc 2');
\x on
select * from train_view_summary;

SELECT assert
(
    dependent_varname           = 'class' AND
    id_col_name                 = 'id' AND
    source_table                = 'dt_golf' AND
    total_rows_processed        = 16 AND
    total_rows_skipped          = 0 AND
    n_folds                     = 0 AND
    null_proxy                  is NULL AND
    num_groups                  = 1 AND
    grouping_cols               = '' AND
    list_of_features_to_exclude is NULL AND
    num_all_groups              = 1 AND
    dependent_var_type          = 'text' AND
    is_classification           = 't' AND
    method                      = 'tree_train' AND
    num_failed_groups           = 0 AND
    input_cp                    = 0 AND
    model_table                 = 'train_view' AND
    list_of_features            = '"OUTLOOK", temperature' AND
    con_features                = '{temperature}' AND
    cat_features                = $${"\"OUTLOOK\""}$$ AND
    independent_varnames        = '{"\"OUTLOOK\"",temperature}' AND
    independent_var_types       = $${text,"double precision"}$$ AND
    dependent_var_levels        = $${"Don't Play",Play}$$,
    'Summary Validation failed for special chars. Actual:' || summary
) from (select * from train_view_summary) summary;

SELECT model_id, model_metadata, model_data, model_type, metrics FROM train_repo;

-- run variable importance using the view.
DROP TABLE IF EXISTS var_output;
SELECT get_var_importance('train_view', 'var_output');
-- predict using the latest view.
DROP TABLE IF EXISTS predict_output;
SELECT tree_predict('train_view', 'dt_golf', 'predict_output');
-- TODO assert output of predict

-- predict using model repo table, and a model id.
DROP TABLE IF EXISTS predict_output;
select tree_predict('train_repo', 'dt_golf', 'predict_output', NULL, 2);

SELECT assert(cat_n_levels[4] = 1,
              'Categorical features with single level not being retained.')
FROM train_view;

SELECT assert(model_count = 2 , 'Expected 2 models in the repo table.') FROM (SELECT count(*) AS model_count FROM train_repo) s;
SELECT assert(max_model_id = 2 , 'The highest model_id should have been 2.') FROM (SELECT max(model_id) AS max_model_id FROM train_repo) s;

-- Test prediction with probability
DROP TABLE IF EXISTS train_output, train_output_summary;
SELECT madlib.tree_train('dt_golf',
'train_view',
'id',
'class',
'"OUTLOOK", temperature, windy',
NULL,
'gini',
NULL,
NULL,
5, 3, 1, 10,NULL, NULL,NULL,'train_repo','name 3','desc 3');

DROP TABLE IF EXISTS prediction_results;
SELECT madlib.tree_predict('train_view', 'dt_golf', 'prediction_results', 'prob');
DROP TABLE IF EXISTS prediction_results;
SELECT madlib.tree_predict('train_repo', 'dt_golf', 'prediction_results', 'prob', 3);
