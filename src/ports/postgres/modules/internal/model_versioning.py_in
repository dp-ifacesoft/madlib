# coding=utf-8
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

import json
import plpy
import types

from internal.model_versioning_helper import *
from utilities.utilities import _assert
from utilities.utilities import add_postfix
from utilities.utilities import BYTEA, BYTEA8
from utilities.utilities import is_platform_pg
from utilities.utilities import is_valid_psql_type
from utilities.utilities import madlib_version
from utilities.utilities import ONLY_ARRAY
from utilities.utilities import unique_string
from utilities.validate_args import get_cols
from utilities.validate_args import quote_ident
from internal.db_utils import quote_dict_values
from utilities.validate_args import table_exists
from utilities.validate_args import TABLES_ONLY, VIEWS_ONLY

m4_changequote(`<!', `!>')

def get_summary_tablename(tbl):
    return add_postfix(tbl, '_summary') if tbl else None

class VersionedModelTableValidator(object):
    """
        Class to validate model tables. This can validate both regular model
        tables, and also versioned model tables.
    """
    def __init__(self, out_table=None, model_repository_table=None,
                 model_id=None):
        """
            Args:
                @params out_table, str: Name of the output table
        """
        if not out_table and not model_repository_table:
            plpy.error("Error: Either the output table/view, " \
                "or the model repository table param must be specified.")
        self.out_table = out_table
        self.out_table_summary = get_summary_tablename(self.out_table)
        self.model_repo_table = model_repository_table
        self.model_repo_table_summary = get_summary_tablename(
            self.model_repo_table)
        self.model_id = model_id
        # Used during training. Set to True if model repo table has to be created
        self.repo_tables_exist = True
        # Used during predict. Set to True, if model repo table and a model id
        # are provided, and a view has to be created and used as model table,
        # False otherwise.
        self.create_view_from_repo_table = False

    def _is_valid_versioned_table(self, cols, expected_cols):
        return len(cols)==len(expected_cols) and \
               all(colname in expected_cols for colname in cols)

    def _validate_model_repo_tables(self):
        """
            Validate model repo and summary tables. If they exist, they are
            valid only if they contain some expected columns. If they don't
            exist, set a flag to indicate the same so that it can be created.
        """
        model_repo_table_exists = table_exists(self.model_repo_table)
        model_repo_table_summary_exists = table_exists(
            self.model_repo_table_summary)
        if model_repo_table_exists and model_repo_table_summary_exists:
            # Check if the two tables have specific columns.
            # First check for model_repo_table cols and types.
            model_repo_table_cols = get_cols(self.model_repo_table)
            _assert(self._is_valid_versioned_table(
                model_repo_table_cols, ModelRepoCols.MODEL_TABLE_COLS),
                "Unexpected columns found in model repo table {0}: {1}.".
                    format(self.model_repo_table, model_repo_table_cols))

            # Now check for self.model_repo_table_summary cols and types.
            model_repo_table_summary_cols = get_cols(self.model_repo_table_summary)
            _assert(self._is_valid_versioned_table(
                model_repo_table_summary_cols, ModelRepoSummaryCols.MODEL_TABLE_SUMMARY_COLS_TYPES),
                "Unexpected columns found in model repo summary table {0}.".
                    format(self.model_repo_table_summary))
        elif model_repo_table_exists or model_repo_table_summary_exists:
            # If only one of the two tables exists, it is invalid.
            plpy.error("Table {0} does not exist.".format(
                self.model_repo_table_summary if model_repo_table_exists
                    else self.model_repo_table))
        else:
            self.repo_tables_exist = False

    def validate_model_tables_for_predict(self):
        """
            This validate function must be called during predict, i.e., when
            we want to read from an existing model, or repository table. So
            validation here is based on the requirements for the same.
        """
        # For reader, we will assume that only out_table is set.
        if self.model_id:
            # The output table specified during predict could be one of three
            # values:
            # 1) a view pointing to a model_id in repo table
            # 2) a repo table
            # 3) a regular non-versioned output table.
            # ONLY in case 2 should model_id be set, we must error out
            # otherwise. So, if model_id is specified, we assume that the
            # out_table passed by the calling function is actually the repo
            # table. So, set the variables accordingly and validate.
            self.model_repo_table = self.out_table
            self.model_repo_table_summary = self.out_table_summary

            # Assert that model_repo_table is NOT a view. It should only be a
            # regular table.
            is_model_repo_a_view = table_exists(self.model_repo_table,
                                                relkind=VIEWS_ONLY)
            is_model_repo_summary_a_view = table_exists(
                self.model_repo_table_summary, relkind=VIEWS_ONLY)
            if is_model_repo_a_view or is_model_repo_summary_a_view:
                plpy.error("Model Versioning Error: The repo and summary tables " \
                           "are expected to be tables and not views.")

            self._validate_model_repo_tables()
            if not self.repo_tables_exist:
                plpy.error("Model Versioning Error: The repository tables "\
                    "{0} and/or {1} do not exist.".format(
                        self.model_repo_table, self.model_repo_table_summary))
            self.create_view_from_repo_table = True
        else:
            # If model_id is not specified, out_table could either be a regular
            # table, or a view pointing to some model_id in some repo table.
            # Assert that tables/views named out_table and out_table_summary
            # exist.
            _assert(table_exists(self.out_table),
                "Given model table/view {} does not exist.".format(self.out_table))
            _assert(table_exists(self.out_table_summary),
                "Given model summary table/view {} does not exist.".format(
                    self.out_table_summary))

    def validate_model_tables_for_view_creator(self):
        if self.model_id:
            # Validate the repository tables.
            self._validate_model_repo_tables()
        else:
            plpy.error("Please provide a model_id to create a view for.")

    def validate_model_tables_for_train(self):
        """
        Validate out_table and model_repository_table parameters for training:
        Check whether model_table and model_table_summary are valid versioned
        model tables.  Returns if valid or non-existent, otherwise throws
        exception. If model_repository_table is not specified, then check
        for the validity of the out_table.
        """
        if self.model_repo_table:
            self._validate_model_repo_tables()
            # Validate self.out_table. This could either be intended to be a
            # table, or a view. If model_repository_table is specified,
            # self.out_table is assumed to be a view. So no table by that name
            # should already exist (view can exist, we drop and re-create them.)
            _assert(not table_exists(self.out_table,
                                     only_first_schema=True,
                                     relkind=TABLES_ONLY),
                "Error: A table named {0} already exists. The model is "\
                "expected to be versioned in {1}, while creating a view "\
                "named {0} pointing to the model.".format(
                    self.out_table, self.model_repo_table))
            _assert(not table_exists(self.out_table_summary,
                                     only_first_schema=True,
                                     relkind=TABLES_ONLY),
                "Error: A table named {0} already exists. The summary of "\
                "the model is expected to be versioned in {1}, while creating "\
                "a view named {0} pointing to the model summary.".format(
                    self.out_table, self.model_repo_table_summary))
        else:
            # If self.model_repo_table does not exist, then neither a view nor
            # a table named self.out_table/self.out_table_summary should exist.
            _assert(not table_exists(
                    self.out_table, only_first_schema=True),
                "Error: Output table {0} already exists.".format(
                    self.out_table))
            _assert(not table_exists(
                    self.out_table_summary, only_first_schema=True),
                "Error: Output summary table {0} already exists.".format(
                    self.out_table_summary))

class VersionedModelViewCreator(object):
    """
        Class to create a view, given a model_id and a model repo table.
        create_view() in this class creates two views for a model_id in
        a model repo table. One view corresponds to the model table, and
        the other to model summary table.
    """
    def __init__(self, schema_madlib, model_repo_table, model_id, view_name,
                 validate_tables=True):
        """
            Initialize necessary parameters for view creation.
            Args:
                @params schema_madlib, str: schema where madlib is installed
                @params model_repo_table, str: Name of the model repository table
                @params model_id, int: Model ID to create view for in the
                            repository table
                @params view_name, str: Name of the output view that would
                            correspond to the model table. Another view with
                            '_summary'suffixed to view_name will be created for
                            the corresponding summary table.
                @params model_data_type, str: The type of the column of the
                            model_data in the view. Default type is double
                            precision[].
                @params validate_tables, bool (optional): Do not vaalidate
                            repo tables if flag is set to False. Default True.
        """
        self.schema_madlib = schema_madlib
        self.model_repo_table = model_repo_table
        self.model_id = model_id
        self.repo_view_name = view_name
        self.summary_view_name = get_summary_tablename(self.repo_view_name)
        self.model_repo_summary_table = get_summary_tablename(self.model_repo_table)
        model_validator = VersionedModelTableValidator(
            model_repository_table=self.model_repo_table,
            model_id=self.model_id)
        if validate_tables:
            model_validator.validate_model_tables_for_view_creator()

    def _get_view_creation_header(self, input_table_name, row_name, view_name):
        """
            Return part of the SQL (header) necessary for creating view.
        """
        model_id_colname = ModelRepoSummaryCols.MODEL_ID_COLNAME
        view_header = """
            DROP VIEW IF EXISTS {view_name};
            CREATE VIEW {view_name} AS
            WITH {row_name} AS (
                SELECT * FROM {input_table_name}
                WHERE {model_id_colname}={model_id})
        """.format(model_id=self.model_id, **locals())
        return view_header

    def create_view(self, model_attributes_types_dict=None,
                    summary_colstypes_dict=None):
        """
            Create views for model table and summary table. The columns in the
            views are mostly created from various JSON blobs in the model
            repository and summary tables. The only column that is not created
            from a JSON element is the model_data column in the model table
            view. This function tries to automatically figure out the types
            of columns created for each key in any given JSON blob. The types
            of the columns in the resulting views can be overridden if need be.
            Args:
                @params model_metadata_types_dict, dict: Override types for
                    keys in the model_metadata JSON column in model repository
                    table.
                @params metrics_colstypes_dict, dict: Override types for keys
                    in the metrics JSON column in model repository table.
                @params summary_colstypes_dict, dict: Override types for keys
                    in the summary JSON column in model repository summary table.
        """
        # Get the necessary JSONs from model repository and summary tables.
        model_attrs_json, model_summary_json = self._get_model_tables_jsons()
        # Find the types of various columns to create from each JSON blob.
        # Override the types of automatically found column types if applicable.
        summary_colstypes_dict = self._get_view_cols_types_for_json(
            model_summary_json['summary'], summary_colstypes_dict)
        model_attributes_types_dict = self._get_view_cols_types_for_json(
            model_attrs_json['attributes'], model_attributes_types_dict)

        # Create view for the model table. Necessary data could be stored in
        # multiple JSON columns such as model_metadata and attributes. We also
        # need to include model_data column in the view, which is handled
        # in _gen_create_view_sql.

        repo_view_sql = self._gen_create_view_sql(
            self.model_repo_table, self.repo_view_name,
            attributes=model_attributes_types_dict)
        plpy.execute(repo_view_sql)
        # Create view for the summary table. All necessary data is stored
        # in the summary JSON column of the repo_summary table.
        summary_view_sql = self._gen_create_view_sql(
            self.model_repo_summary_table, self.summary_view_name,
            summary=summary_colstypes_dict)
        plpy.execute(summary_view_sql)


    def _gen_create_view_sql(self, table_name, view_name,
                             **json_colstypes_dicts):
        """
            Generate SQL statements for creating columns and populating its
            values from JSON blobs.
            Args:
                @params table_name, str: Name of the table containing the JSON blob
                @params view_name, str: Output view to create
                @params **json_colstypes_dicts: JSON blobs as python dicts.
            Returns:
                Part of the view creation SQL query. This part deals with
                the various select clauses necessary in creating the view.
        """
        model_row = unique_string(desp='model_row')
        select_clause = self._get_view_creation_header(
            table_name, model_row, view_name)

        # All the data necessary to create view is in various JSONs. Each key
        # in a JSON will be converted to a column in the view, with the value
        # equal to the corresponding value in the JSON object.
        # First process all necessary JSON columns in the repo/repo_summary
        # tables to create corresponding columns in views.
        scalar_cols_select_list = []
        array_colnames_clause_list = []
        array_colnames_list = []
        for column, json_colstypes_dict in json_colstypes_dicts.iteritems():
            # column is the name of the JSON column in table_name
            # json_colstypes_dict is the JSON blob in column as a python dict

            # First get all the select clauses from scalar values in the JSON
            scalar_cols_select_list += self._get_scalar_cols_select_sql_list(
                column, json_colstypes_dict, model_row)
            # Get all the select clauses from array values in the JSON.
            colnames_select_sql, colnames = self._get_array_cols_select_sql_list(
                column, json_colstypes_dict, model_row)
            array_colnames_clause_list += colnames_select_sql
            array_colnames_list += colnames

        ## ----------------------------------------------------------------- ##
        ## ----------------------------------------------------------------- ##
        # Stitch various components together. Example query to create view
        # from model_repo_table for a Decision Tree model:
        # drop view if exists latest_model;
        # create view latest_model as
        #     with latest as (select * from train_output order by model_id = 1)
        #     select
        #         (latest.attributes->>'pruning_cp')::int as pruning_cp,
        #         (latest.attributes->>'tree_depth')::int as tree_depth
        #         latest.model_type,
        #         madlib.bytea_to_bytea8(latest.model_data)::madlib.bytea8 as model_data,
        #         a1.cat_n_levels,
        #         a2.cat_levels_in_text,
        #         a3.impurity_var_importance,
        #     from
        #         (select array_agg(v) as cat_n_levels from
        #             (select json_array_elements(
        #                    attributes->'cat_n_levels')::text::int as v from latest
        #             ) as aa1
        #         ) as a1,
        #         (select array_agg(v) as cat_levels_in_text from
        #             (select ('[' || json_array_elements( attributes->'cat_levels_in_text') || ']'
        #                     )::json->>0 as v from latest
        #             ) as aa2
        #         ) as a2,
        #         (select array_agg(v) as impurity_var_importance from
        #             (select json_array_elements(
        #                   attributes->'impurity_var_importance')::text::double precision as v from latest
        #             ) as aa3
        #         ) as a3,
        #         latest;

        # View creation query from summary table is similar to the one with
        # model table as above, it just involves expanding out the `summary`
        # JSON column from the summary repo table into corresponding columns
        # in views.
        ## ----------------------------------------------------------------- ##
        ## ----------------------------------------------------------------- ##

        select_clause += " SELECT "

        # If table_name is the repository table, then we must also create a
        # column for the model_data column. This is not a part of any JSON blob
        # in the repo table, so handle it separately. The model must be type
        # casted to the correct model_data_type too.
        if table_name == self.model_repo_table:
            select_clause += ModelRepoCols.MODEL_TYPE_COLNAME
            select_clause += ', ' + self._get_model_data_select_clause()
        elif table_name == self.model_repo_summary_table:
            select_clause += ', '.join(ModelRepoSummaryCols.COLS_FOR_VIEW_FROM_SUMMARY_REPO)

        # Append select statements for all JSON keys associated with scalar values
        select_clause += ', ' + ', '.join(scalar_cols_select_list)
        # If any of the JSON keys had array values, the select statement is
        # slightly more involved since we must use JSON functions to get the
        # values and convert them to arrays that SQL understands.
        if array_colnames_list:
            select_clause += ', ' + ', '.join(array_colnames_list)
            select_clause += ' FROM ' + ', '.join(array_colnames_clause_list)
            select_clause += ', '
        else:
            select_clause += ' FROM '

        select_clause += model_row
        return select_clause

    def _get_model_data_select_clause(self):
        model_data_type = plpy.execute("""
                    SELECT {model_dtype_cname}
                    FROM {model_repo_summary_table}
                    WHERE {model_id_colname}={model_id}
                """.format(
                    model_repo_summary_table=self.model_repo_summary_table,
                    model_id=self.model_id,
                    model_id_colname=ModelRepoSummaryCols.MODEL_ID_COLNAME,
                    model_dtype_cname=ModelRepoSummaryCols.MODEL_DATA_TYPE_COLNAME)
                )[0]['model_data_type']
        if model_data_type == 'bytea8':
            model_data_str = "{schema_madlib}.bytea_to_bytea8({model_data_colname})" \
                " as {model_data_colname}"
        elif model_data_type == 'double precision[]':
            model_data_str = "{schema_madlib}._double_array_recv" \
                "({model_data_colname}, 'double precision'::regtype::int) " \
                "as {model_data_colname}"
        else:
            plpy.error("Invalid model_data_type {}".format(
                self.model_data_type))
        model_data_str = model_data_str.format(
            schema_madlib=self.schema_madlib,
            model_data_colname=ModelRepoCols.MODEL_DATA_COLNAME)
        return model_data_str

    def _get_scalar_cols_select_sql_list(self, json_colname_in_tbl,
                                         json_colstypes_dict, tbl_row):
        """
            Get list of select clauses for scalar values in a JSON blob.
            Args:
                @params json_colname_in_tbl: Name of the JSON column to process
                    in input table.
                @params json_colstypes_dict: Dict containing the type info for each
                    key in the JSON blob. eg:
                    {json_key: {json_value_type: is_json_value_array_flag}}
                @params tbl_row: alias used in the query which has to be used in a
                    SQL select clause to avoid ambiguity.
            Returns: A list of all the select clauses which has to be stitched
                together to form the final query.
        """
        ## ----------------------------------------------------------------- ##
        ## ----------------------------------------------------------------- ##
        # Example of what we want to return for a scalar col, where JSON blob
        # is `attributes`, and the key which contains a scalar value in that
        # blob is `pruning_cp`:
        # select (latest.attributes->>'pruning_cp')::int as pruning_cp
        ## ----------------------------------------------------------------- ##
        ## ----------------------------------------------------------------- ##
        select_scalars_list = ["({0}.{1}->>'{2}')::{3} AS {2}".format(
            tbl_row, json_colname_in_tbl, colname, coltype_info['col_type'])
                for colname, coltype_info in json_colstypes_dict.items()
                if not coltype_info['is_list']]
        return select_scalars_list

    def _get_array_cols_select_sql_list(self, json_colname_in_tbl,
                                        json_colstypes_dict, tbl_row):
        """
            Get lists of select clauses for scalar values in a JSON blob.
            Args:
                @params json_colname_in_tbl: Name of the JSON column to process in
                    input table.
                @params json_colstypes_dict: Dict containing the type info for each
                    key in the JSON blob. eg:
                    {json_key: {json_value_type: is_json_value_array_flag}}
                @params tbl_row: alias used in the query which has to be used in a
                    SQL select clause to avoid ambiguity.
            Returns:
                This function returns two lists, one containing all the
                sub-queries for selecting arrays from JSON blobs, and another
                list with just select statements referring to each inner
                query's alias.
        """
        ## ----------------------------------------------------------------- ##
        ## ----------------------------------------------------------------- ##
        # Example of what we want to return for an INTEGER array from a JSON
        # blob, where the json column is `metrics`, and the key in the JSON
        # blob which contains an INTEGER array value is `cat_n_levels`:
        # (select array_agg(v) as cat_n_levels from
        #     (select json_array_elements( metrics->'cat_n_levels'
        #                                )::text::int as v from latest
        #     ) as aa1
        # ) as a1,

        # Example of what we want to return for a TEXT array from a json
        # object, where the json column is `attributes`, and the key in
        # the blob that contains a TEXT array value is `cat_levels_in_text`:
        # (select array_agg(v) as cat_levels_in_text from
        #     (select ('[' || json_array_elements( attributes->'cat_levels_in_text') || ']'
        #             )::json->>0 as v from latest
        #     ) as aa2
        # ) as a2,
        ## ----------------------------------------------------------------- ##
        ## ----------------------------------------------------------------- ##

        # Strip out the '[]' from coltype. '[]' should not be specified, since
        # array_cols[i]['ctype'] is used to typecast the output of
        # json_array_elements(), which is a scalar. These individual elements
        # are passed to array_agg to create the actual array in the query.
        array_cols = [
            {'cname':colname, 'ctype':coltype_info['col_type'].strip()[:-2]}
            for colname, coltype_info in json_colstypes_dict.items()
            if coltype_info['is_list']
        ]
        subquery_names = [unique_string(desp='subq_{0}'.format(i))
            for i in range(len(array_cols))]
        select_colnames_list = ["{0}.{1}".format(
                subquery_names[i], array_cols[i]['cname'])
            for i in range(len(subquery_names))]
        inner_query_name = unique_string(desp='inner')
        json_temp_name = unique_string(desp='v')

        def _get_sql(array_type):
            if array_type == 'text':
                # Note that {3} is not used in this string. While converting
                # from a text array, we don't have to typecast to TEXT since
                # it is already of type text.
                select_clause = """(SELECT array_agg({0}) AS {1} FROM (
                    SELECT ('[' || json_array_elements({2}->'{1}') || ']')::json->>0 AS {0}
                    FROM {4}) AS {5}) AS {6}"""
            else:
                select_clause = """(SELECT array_agg({0}) AS {1} FROM (
                    SELECT json_array_elements({2}->'{1}')::TEXT::{3} AS {0}
                    FROM {4}) AS {5}) AS {6}"""
            return select_clause

        select_colnames_clause_list = [_get_sql(array_cols[i]['ctype']).format(
                json_temp_name, # {0} alias used in query
                array_cols[i]['cname'], # {1} name of column to include in view
                json_colname_in_tbl,# {2} name of the json column in input tbl
                array_cols[i]['ctype'], # {3} type of column included in view
                tbl_row, # {4} alias used for row corresponding to model_id in input tbl
                inner_query_name, # {5} alias for inner query
                subquery_names[i]) # {6} alias for outer sub query
            for i in range(len(array_cols))]

        return select_colnames_clause_list, select_colnames_list


    def _get_view_cols_types_for_json(self,
                                      json_from_table,
                                      type_overriding_dict):
        """
            Get the column names, and their types to be created in the
            view. These column names and types are obtained from JSON blobs
            in the model repo/summary tables.
            Args:
                @params json_from_table, str: JSON blob in the repo/summary
                    table as a python dict.
                @params type_overriding_dict, dict: Types are automatically
                    figures out in this function. This dict provides the
                    overriding types for column names if need be.
            Returns:
                A dictionary, with the column name being the key, and a dict
                for value containing.
                Example:
                    {colname: {coltype: is_coltype_a_list}}
                colname: is the name of the column in view, which is the key
                    in json
                cotype: is the type of the column
                is_coltype_a_list: is a boolean flag indicating if the type is
                    an array or not.
        """

        def get_type(col_name, col_value):
            """
                    Given JSON key-value pair, return back the column to create
                    in the view, along with its type. key represents the column
                    name, and the type of value in python is mapped to a sql type.
                """
            # Map types in python to types in SQL. Couple points to note:
            # 1. strings in JSON when read out in python is of type unicode.
            # 2. types.NoneType is to capture NULL in SQL. We will thus end up
            #    casting any column with NULL value to type text.
            python_to_sql_type_map = {
                float:'double precision',
                int:'int',
                unicode:'text',
                types.NoneType:'text',
                bool:'boolean',
            }
            value_type = type(col_value)
            type_suffix = ''
            is_list = False
            if type(col_value) == list:
                # If col_value is a list, set value_type to the type of the first
                # element in the list. All elements in this list should be of
                # the same type.
                value_type = types.NoneType if not col_value else type(col_value[0])
                type_suffix = '[]'
                is_list = True
            # If the calling function has passed a dict to override the types
            # of specific columns, use the overriding types.
            if type_overriding_dict and col_name in type_overriding_dict:
                return {'col_type': type_overriding_dict[col_name],
                        'is_list': is_list}
            else:
                if value_type in python_to_sql_type_map:
                    return {'col_type': python_to_sql_type_map[value_type]+type_suffix,
                            'is_list': is_list}
                else:
                    plpy.error("Unknown type for json key-value ({0}:{1})".
                               format(col_name, value_type))

        view_cols_types = {col_name:get_type(col_name, col_value)
            for col_name, col_value in json.loads(json_from_table).items()}
        return view_cols_types

    def _get_model_tables_jsons(self):
        """
            Return back the JSON columns required to create views from both
            model_repo table and model_repo_summary.
        """
        model_id_colname=ModelRepoSummaryCols.MODEL_ID_COLNAME
        model_attrs_json = plpy.execute("""
                SELECT {attr_colnam}
                FROM {self.model_repo_table}
                WHERE {model_id_colname}={self.model_id}
            """.format(self=self,
                       attr_colnam=ModelRepoCols.ATTRIBUTES_COLNAME,
                       model_id_colname=model_id_colname))
        if not model_attrs_json:
            plpy.error("Invalid model id ({0}) provided.".format(self.model_id))

        model_summary_json = plpy.execute("""
                SELECT {summary_colname}
                FROM {self.model_repo_summary_table}
                WHERE {model_id_colname}={self.model_id}
            """.format(self=self,
                       summary_colname=ModelRepoSummaryCols.SUMMARY_COLNAME,
                       model_id_colname=model_id_colname))
        if not model_summary_json:
            plpy.error("Missing summary table for {0} {1}.".format(
                model_id_colname, self.model_id))

        return model_attrs_json[0], model_summary_json[0]

class VersionedModelWriter(object):
    def __init__(self, schema_madlib, model_view, model_repository_table):
        self.schema_madlib = schema_madlib
        self.model_view = model_view
        self.model_view_summary = get_summary_tablename(model_view)
        if model_repository_table:
            self.model_repo_table = model_repository_table
            self.model_repo_table_summary = get_summary_tablename(
                self.model_repo_table)
        else:
            plpy.error("Model Versioning Error: The model_repository_table " \
                "must be specified to write the model to a versioned table.")
        self.MODEL_VER_DELIMITER = '$model_ver$'
        # Validate repo table
        validator = VersionedModelTableValidator(
            self.model_view, self.model_repo_table)
        validator.validate_model_tables_for_train()
        if not validator.repo_tables_exist:
            self._create_model_repo_tables()

    def _create_model_repo_tables(self):
        if is_platform_pg():
            distributed_by_clause = ''
        else:
            distributed_by_clause= 'DISTRIBUTED BY ({})'.format(
                ModelRepoSummaryCols.MODEL_ID_COLNAME)
        model_table_cols = ', '.join(['{0} {1}'.format(colname, coltype)
                    for colname, coltype in
                    ModelRepoCols.MODEL_TABLE_COLS.items()])
        create_model_sql = """
                CREATE TABLE {self.model_repo_table}
                ({model_table_cols})
                {distributed_by_clause}
            """.format(**locals())
        plpy.execute(create_model_sql)

        model_table_summary_cols = ', '.join(['{0} {1}'.format(colname, coltype)
                    for colname, coltype in ModelRepoSummaryCols.MODEL_TABLE_SUMMARY_COLS_TYPES.items()])
        create_model_summary_sql = """
                CREATE TABLE {self.model_repo_table_summary}
                ({model_table_summary_cols})
                {distributed_by_clause}
           """.format(model_repo_table_summary=self.model_repo_table_summary,
                      **locals())
        plpy.execute(create_model_summary_sql)

    def store_model(self, model_data, model_attributes_json_dict,
                    repo_summary_cols_dict, model_summary_json_dict):
        """
            Add new rows into model repository and model repo summary
            tables. This function also creates a view named model_view
            that points to the new model that is inserted by this function.
            Args:
                @params model_data, double array/bytea8: The actual model
                    (coefficients)
                @params model_attributes_json_dict, dict: Dict containing key
                    value that will be converted to a JSON col named
                    attributes in the repo table.
                @params repo_summary_cols_dict, dict: Dict capturing the
                    values for all other non-JSON columns in the summary
                    repo table.
                @params model_summary_json_dict, dict: Dict containing key
                    value that will be converted to a JSON col named summary
                    in the summary repo table.
        """
        # Get value of model_type before quoting it in function
        # _store_model_table_summary()
        self._validate_repo_summary_cols(repo_summary_cols_dict)
        model_type_colname = ModelRepoSummaryCols.MODEL_TYPE_COLNAME
        if repo_summary_cols_dict[model_type_colname]:
            self.model_type = repo_summary_cols_dict[model_type_colname].strip()
        else:
            plpy.error("Model Versioning Error: Invalid model type.")
        # Get value of model_data_type before quoting it in function
        # _store_model_table_summary()
        model_dtype_colname = ModelRepoSummaryCols.MODEL_DATA_TYPE_COLNAME
        if repo_summary_cols_dict[model_dtype_colname]:
            self.model_data_type = repo_summary_cols_dict[model_dtype_colname].strip()
        else:
            plpy.error("Model Versioning Error: Invalid model data type.")
        # We need a unique model id for each model train call (for every
        # such call, a single row is inserted into the summary table,
        # but  in case of grouping, multiple rows could be inserted into the
        # model repo table with the same model id).
        # We therefore first insert into the summary table that has a serial
        # model_id column, along with an internal MADlib model id that is
        # created by us and guaranteed to be unique. We then use this internal
        # MADlib model id to get the serial model_id value. This serial value
        # is written into the model repo table. Note that model_id column in the
        # repo table is NOT serial.

        model_id = self._store_model_table_summary(
            repo_summary_cols_dict, model_summary_json_dict)
        self._store_model_table(
            model_id, model_data, model_attributes_json_dict)

        view_creator = VersionedModelViewCreator(self.schema_madlib,
                                                 self.model_repo_table,
                                                 model_id,
                                                 self.model_view,
                                                 validate_tables=False)
        view_creator.create_view()

    def _validate_repo_summary_cols(self, summary_cols_from_module):
        missing_cols = [k for k in summary_cols_from_module.keys()
            if k not in
            ModelRepoSummaryCols.SUMMARY_REPO_COLS_POPULATED_BY_MODULES
            ]
        if missing_cols:
            plpy.error("Model Versioning Error: Summary columns {} " \
                "are not assigned any value.".format(missing_cols))


    def _store_model_table_summary(self, repo_summary_cols_dict,
                                   model_summary_json):
        # Get names of keys we want to use/update in repo_summary_cols_dict.
        # These keys are essentially column names in summary table repo.
        model_id_colname = ModelRepoSummaryCols.MODEL_ID_COLNAME
        internal_id_colname = ModelRepoSummaryCols.INTERNAL_ID_COLNAME
        summary_colname = ModelRepoSummaryCols.SUMMARY_COLNAME
        model_tbl_colname = ModelRepoSummaryCols.MODEL_TABLE_COLNAME
        model_repo_colname = ModelRepoSummaryCols.MODEL_REPO_COLNAME
        mad_ver_colname = ModelRepoSummaryCols.MAD_VER_COLNAME
        ind_var_colname = ModelRepoSummaryCols.INDEPENDENT_COLNAME
        # Quote all the summary table column values. Add keys to the dict that
        # were not passed in.
        repo_summary_cols_dict[summary_colname] = self._to_json(model_summary_json)
        repo_summary_cols_dict[model_tbl_colname] = self.model_view
        repo_summary_cols_dict[model_repo_colname] = self.model_repo_table
        repo_summary_cols_dict[mad_ver_colname] = madlib_version(
            self.schema_madlib)
        repo_summary_cols_dict[internal_id_colname] = unique_string(prefix_has_temp=False)
        # Do not quote list type columns.
        quote_dict_values(repo_summary_cols_dict,
                          [model_id_colname, ind_var_colname],
                          self.MODEL_VER_DELIMITER)

        # For now the calling function is expected to handle
        # special characters for all text columns in the summary repo table.
        # do not insert model_id value because it is serial
        summary_table_cols = ', '.join(['{0}'.format(colname)
            for colname in ModelRepoSummaryCols.MODEL_TABLE_SUMMARY_COLS_TYPES if colname != model_id_colname])
        repo_summary_cols_dict.update(locals())
        sql = """
            INSERT INTO {self.model_repo_table_summary} ({summary_table_cols})
            VALUES ({model_type},
                    {start_training_time},
                    {end_training_time},
                    {source_table},
                    {model_table},
                    {model_repository},
                    {dependent_varname},
                    {independent_varname},
                    {grouping_col},
                    {name},
                    {description},
                    {madlib_version},
                    {summary},
                    {model_data_type},
                    {__madlib_internal_id__})
        """.format(MODEL_VER_DELIMITER=self.MODEL_VER_DELIMITER,
                   **repo_summary_cols_dict)
        plpy.execute(sql)

        model_id = plpy.execute("""
                SELECT {model_id_colname}
                FROM {model_repo_table_summary}
                WHERE {internal_id_colname}={madlib_internal_id}
            """.format(
                model_id_colname=model_id_colname,
                model_repo_table_summary=self.model_repo_table_summary,
                internal_id_colname=internal_id_colname,
                madlib_internal_id=repo_summary_cols_dict[internal_id_colname])
        )[0][model_id_colname]
        return model_id

    def _store_model_table(self, model_id, model_data, model_attributes):
        model_attributes = self._to_json(model_attributes)
        if is_valid_psql_type(self.model_data_type, ONLY_ARRAY) \
                and type(model_data) == list:
            #TODO may have performance problems. compare with writing this to a table
            model_data_query = 'array_send(ARRAY{0})'.format(model_data)
        elif is_valid_psql_type(self.model_data_type, BYTEA8):
            model_data_query = "{self.schema_madlib}.bytea8send($madlib${model_data}$madlib$)".\
                format(**locals())
        else:
            plpy.error('Cannot write model to {0}. Unsupported model data type {1}'.\
                format(self.model_repo_table, self.model_data_type))

        model_table_cols = ', '.join(['{0}'.format(colname)
                    for colname in ModelRepoCols.MODEL_TABLE_COLS])
        sql = """INSERT INTO {self.model_repo_table} ({model_table_cols})
                 VALUES ('{model_id}', '{self.model_type}', NULL,
                         {model_data_query},
                         {self.MODEL_VER_DELIMITER}{model_attributes}{self.MODEL_VER_DELIMITER})
        """.format(**locals())
        plpy.execute(sql)

    def _to_json(self, json_dict):
        if type(json_dict) == dict:
            return json.dumps(json_dict)
        return '{}'

class VersionedModelReader():
    """
        Class that exposes functions to read models from repository table.
    """
    def __init__(self, schema_madlib, model_table, model_id):
        self.schema_madlib = schema_madlib
        self.model_table = model_table
        self.model_table_summary = get_summary_tablename(self.model_table)
        self.model_id = model_id
        self.drop_temp_model_views = False
        self.model_to_use = self.model_table
        self.model_summary_to_use = self.model_table_summary
        self.validator = VersionedModelTableValidator(
            self.model_table, model_id=self.model_id)
        self.validator.validate_model_tables_for_predict()

    def get_model_tables_for_prediction(self):
        """
            This function creates two temporary views, one from model repo
            table, and another from model repo summary table for a given
            model_id. This function is typically called from predict
            functions of modules that support model versioning.
        """
        if self.validator.create_view_from_repo_table:
            self.model_to_use = unique_string(desp='model_view')
            self.model_summary_to_use = get_summary_tablename(self.model_to_use)
            view_creator = VersionedModelViewCreator(self.schema_madlib,
                                                     self.model_table,
                                                     self.model_id,
                                                     self.model_to_use)
            view_creator.create_view()
            self.drop_temp_model_views = True
        return self.model_to_use, self.model_summary_to_use

    def _from_json(json_str):
        json_dict = dict()
        try:
            json_dict = json.loads(json_str)
        except:
            plpy.error("Invalid json string, cannot deserialize it.")
        return json_dict

    def __del__(self):
        """
            This function cleans up temporary views created by
            get_model_tables_for_prediction().
        """
        if self.drop_temp_model_views:
            plpy.execute("""
                    DROP VIEW IF EXISTS {0}, {1}
                """.format(self.model_to_use, self.model_summary_to_use))

    # def _get_model_id(self, model_id):
    #     if model_id:
    #         return model_id
    #     return plpy.execute("""
    #        SELECT max(model_id) AS id FROM {self.model_table_summary}
    #        """.format(self=self))[0]['id']

    # def _read_all_data(self):
    #     self.model_data_dict = self._run_select_query(self.model_table)
    #     self.model_summary_dict = self._run_select_query(self.model_table_summary)

    # def _run_select_query(self, table):
    #     if is_valid_psql_type(self.model_data_type, ONLY_ARRAY):
    #         model_data_query = '{schema_madlib}._double_array_recv(model_data)'.\
    #             format(self.schema_madlib)
    #     elif is_valid_psql_type(self.model_data_type, BYTEA8):
    #         model_data_query = "{schema_madlib}.bytea_to_bytea8(model_data)".\
    #             format(self.schema_madlib)
    #     else:
    #         plpy.error("Cannot read model from model table. Unsupported "
    #             "model data type {}".format(self.model_data_type))

    #     res = plpy.execute("""
    #                 SELECT model_id, metrics, model_metadata, model_type, {model_data_query}
    #                 FROM {table}
    #                 WHERE model_id={model_id}
    #             """.format(model_id=self.model_id, **locals()))
    #     if not res:
    #         plpy.error("Model id ({0}) does not exist in {1}.".format(
    #             self.model_id, table))
    #     # Note that since this version does not support grouping, we can
    #     # assume exactly one row to be the output of this query. Will change
    #     # once grouping comes in.
    #     return res[0]

    # def get_model_data(self):
    #     return self.model_data_dict['model_data']

    # def get_metrics(self):
    #     return self._from_json(self.model_data_dict['metrics'])

    # def get_model_metadata(self):
    #     return self._from_json(self.model_data_dict['model_metadata'])

    # def get_summary(self):
    #     return self._from_json(self.model_summary_dict['summary'])

